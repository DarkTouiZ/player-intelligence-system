{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120121,"databundleVersionId":14583263,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VVS-XGBoost","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/cpe342-karena/public_dataset/task1/train.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:28:18.795739Z","iopub.execute_input":"2025-11-28T19:28:18.796684Z","iopub.status.idle":"2025-11-28T19:28:19.564079Z","shell.execute_reply.started":"2025-11-28T19:28:18.796656Z","shell.execute_reply":"2025-11-28T19:28:19.562932Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from ydata_profiling import ProfileReport\n\nprofile = ProfileReport(df, explorative = True)\nprofile.to_file(\"Task1ProfileReport.html\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:28:19.565753Z","iopub.execute_input":"2025-11-28T19:28:19.566139Z","iopub.status.idle":"2025-11-28T19:31:52.178275Z","shell.execute_reply.started":"2025-11-28T19:28:19.566108Z","shell.execute_reply":"2025-11-28T19:31:52.177164Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b826accbf494dcda615236a48cd1da2"}},"metadata":{}},{"name":"stderr","text":"\n  0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\n  3%|▎         | 1/34 [00:01<01:03,  1.93s/it]\u001b[A\n 15%|█▍        | 5/34 [00:02<00:10,  2.82it/s]\u001b[A\n 26%|██▋       | 9/34 [00:02<00:04,  5.04it/s]\u001b[A\n 44%|████▍     | 15/34 [00:02<00:02,  7.98it/s]\u001b[A\n 56%|█████▌    | 19/34 [00:03<00:01,  9.01it/s]\u001b[A\n 68%|██████▊   | 23/34 [00:03<00:01,  9.89it/s]\u001b[A\n 76%|███████▋  | 26/34 [00:03<00:00, 11.63it/s]\u001b[A\n 82%|████████▏ | 28/34 [00:03<00:00, 11.19it/s]\u001b[A\n 88%|████████▊ | 30/34 [00:04<00:00, 11.72it/s]\u001b[A\n100%|██████████| 34/34 [00:04<00:00,  7.72it/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0eecbd9e6d494d41a9bbd35209428c59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f5d7adcb40485e91fb8e905ae39bff"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4709716221d14dabafdda35180be3879"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import numpy as np\n\ndf.drop(columns = [\"id\", \"player_id\"], inplace = True)\ndf = df.dropna(subset = [\"is_cheater\"])\n\nfrom xgboost import XGBClassifier, DMatrix, train as xgb_train\n\nX = df.drop(\"is_cheater\", axis = 1)\ny = df[\"is_cheater\"]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = 0.2, random_state = 42, stratify = y\n)\n\n# native XGBoost\ndmatrix = DMatrix(X_train, label = y_train, missing = np.nan)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:31:52.180296Z","iopub.execute_input":"2025-11-28T19:31:52.180655Z","iopub.status.idle":"2025-11-28T19:31:52.311429Z","shell.execute_reply.started":"2025-11-28T19:31:52.180632Z","shell.execute_reply":"2025-11-28T19:31:52.310391Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"model = XGBClassifier(\n    objective = \"binary:logistic\",\n    eval_metric = \"logloss\",\n    use_label_encoder = False\n)\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\nprint(\"Predictions:\", y_pred)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:31:52.312341Z","iopub.execute_input":"2025-11-28T19:31:52.313020Z","iopub.status.idle":"2025-11-28T19:31:54.142733Z","shell.execute_reply.started":"2025-11-28T19:31:52.312992Z","shell.execute_reply":"2025-11-28T19:31:54.142049Z"}},"outputs":[{"name":"stdout","text":"Predictions: [1 0 0 ... 0 0 0]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from sklearn.metrics import fbeta_score\n\nf2_macro = fbeta_score(y_test, y_pred, beta = 2, average = \"macro\")\nf2_micro = fbeta_score(y_test, y_pred, beta = 2, average = \"micro\")\nf2_weighted = fbeta_score(y_test, y_pred, beta = 2, average = \"weighted\")\n\nprint(f\"F2 Macro: {f2_macro:.4f}\")\nprint(f\"F2 Micro: {f2_micro:.4f}\")\nprint(f\"F2 Weighted: {f2_weighted:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:31:54.144890Z","iopub.execute_input":"2025-11-28T19:31:54.145322Z","iopub.status.idle":"2025-11-28T19:31:54.194396Z","shell.execute_reply.started":"2025-11-28T19:31:54.145282Z","shell.execute_reply":"2025-11-28T19:31:54.193325Z"}},"outputs":[{"name":"stdout","text":"F2 Macro: 0.7644\nF2 Micro: 0.8041\nF2 Weighted: 0.8007\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"importance_scores = model.feature_importances_\n\nfeat_importance = pd.DataFrame({\n    \"feature\": X.columns,\n    \"importance\": importance_scores\n}).sort_values(by = \"importance\", ascending = False)\n\nfeat_importance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:31:54.195317Z","iopub.execute_input":"2025-11-28T19:31:54.195592Z","iopub.status.idle":"2025-11-28T19:31:54.209785Z","shell.execute_reply.started":"2025-11-28T19:31:54.195566Z","shell.execute_reply":"2025-11-28T19:31:54.208755Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                    feature  importance\n30      crosshair_placement    0.228502\n10         reports_received    0.195548\n6          account_age_days    0.101007\n9       friend_network_size    0.092826\n7                     level    0.061533\n0          kill_death_ratio    0.031289\n1       headshot_percentage    0.027607\n20         game_sense_score    0.023645\n3            accuracy_score    0.017301\n19      spray_control_score    0.015208\n18        aiming_smoothness    0.013739\n8   level_progression_speed    0.013082\n4          kill_consistency    0.012653\n2                  win_rate    0.012331\n21       communication_rate    0.009831\n28         damage_per_round    0.009515\n24            map_knowledge    0.009491\n16      weapon_switch_speed    0.009354\n5          reaction_time_ms    0.009268\n26         first_blood_rate    0.009256\n12  input_consistency_score    0.009225\n13   avg_session_length_min    0.009149\n15         night_play_ratio    0.009138\n27        survival_time_avg    0.009115\n11     device_changes_count    0.008962\n17   movement_pattern_score    0.008957\n23       buy_decision_score    0.008770\n29       utility_usage_rate    0.008687\n25      clutch_success_rate    0.008424\n14         sessions_per_day    0.008360\n22          team_play_score    0.008226","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>30</th>\n      <td>crosshair_placement</td>\n      <td>0.228502</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>reports_received</td>\n      <td>0.195548</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>account_age_days</td>\n      <td>0.101007</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>friend_network_size</td>\n      <td>0.092826</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>level</td>\n      <td>0.061533</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>kill_death_ratio</td>\n      <td>0.031289</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>headshot_percentage</td>\n      <td>0.027607</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>game_sense_score</td>\n      <td>0.023645</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>accuracy_score</td>\n      <td>0.017301</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>spray_control_score</td>\n      <td>0.015208</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>aiming_smoothness</td>\n      <td>0.013739</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>level_progression_speed</td>\n      <td>0.013082</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>kill_consistency</td>\n      <td>0.012653</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>win_rate</td>\n      <td>0.012331</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>communication_rate</td>\n      <td>0.009831</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>damage_per_round</td>\n      <td>0.009515</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>map_knowledge</td>\n      <td>0.009491</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>weapon_switch_speed</td>\n      <td>0.009354</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>reaction_time_ms</td>\n      <td>0.009268</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>first_blood_rate</td>\n      <td>0.009256</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>input_consistency_score</td>\n      <td>0.009225</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>avg_session_length_min</td>\n      <td>0.009149</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>night_play_ratio</td>\n      <td>0.009138</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>survival_time_avg</td>\n      <td>0.009115</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>device_changes_count</td>\n      <td>0.008962</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>movement_pattern_score</td>\n      <td>0.008957</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>buy_decision_score</td>\n      <td>0.008770</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>utility_usage_rate</td>\n      <td>0.008687</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>clutch_success_rate</td>\n      <td>0.008424</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>sessions_per_day</td>\n      <td>0.008360</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>team_play_score</td>\n      <td>0.008226</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"weak_feature = feat_importance[\"feature\"].tail(15).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:31:54.210949Z","iopub.execute_input":"2025-11-28T19:31:54.212155Z","iopub.status.idle":"2025-11-28T19:31:54.227386Z","shell.execute_reply.started":"2025-11-28T19:31:54.212126Z","shell.execute_reply":"2025-11-28T19:31:54.226365Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"X.drop(columns = weak_feature, inplace = True)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = 0.2, random_state = 42, stratify = y\n)\n\ndmatrix = DMatrix(X_train, label = y_train, missing = np.nan)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:31:54.228554Z","iopub.execute_input":"2025-11-28T19:31:54.228816Z","iopub.status.idle":"2025-11-28T19:31:54.316445Z","shell.execute_reply.started":"2025-11-28T19:31:54.228797Z","shell.execute_reply":"2025-11-28T19:31:54.315548Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"model.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nf2_macro = fbeta_score(y_test, y_pred, beta = 2, average = \"macro\")\nf2_micro = fbeta_score(y_test, y_pred, beta = 2, average = \"micro\")\nf2_weighted = fbeta_score(y_test, y_pred, beta = 2, average = \"weighted\")\n\nprint(f\"F2 Macro: {f2_macro:.4f}\")\nprint(f\"F2 Micro: {f2_micro:.4f}\")\nprint(f\"F2 Weighted: {f2_weighted:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:31:54.317144Z","iopub.execute_input":"2025-11-28T19:31:54.317385Z","iopub.status.idle":"2025-11-28T19:31:55.456499Z","shell.execute_reply.started":"2025-11-28T19:31:54.317362Z","shell.execute_reply":"2025-11-28T19:31:55.455358Z"}},"outputs":[{"name":"stdout","text":"F2 Macro: 0.7658\nF2 Micro: 0.8067\nF2 Weighted: 0.8030\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import make_scorer\nimport optuna\n\nf2_macro_scorer = make_scorer(fbeta_score, beta = 2, average = \"macro\")\n\ndef objective(trial):\n    param = {\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log = True),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0, 5),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0, 5),\n        \"use_label_encoder\": False,\n        \"objective\": \"multi:softmax\",\n        \"num_class\": 4,\n        \"eval_metric\": \"mlogloss\",\n        \"n_jobs\": -1,\n        \"random_state\": 42\n    }\n    \n    model = XGBClassifier(**param)\n    \n    # Stratified 5-fold CV\n    cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n    \n    # Evaluate using F2 macro\n    scores = cross_val_score(model, X, y, cv = cv, scoring = f2_macro_scorer)\n    \n    return scores.mean()\n\nstudy = optuna.create_study(direction = \"maximize\")\nstudy.optimize(objective, n_trials = 50)\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(\"F2 Macro:\", trial.value)\nprint(\"Best hyperparameters:\", trial.params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T05:07:17.047437Z","iopub.execute_input":"2025-11-28T05:07:17.047767Z","iopub.status.idle":"2025-11-28T05:30:32.246534Z","shell.execute_reply.started":"2025-11-28T05:07:17.047742Z","shell.execute_reply":"2025-11-28T05:30:32.245738Z"}},"outputs":[{"name":"stderr","text":"[I 2025-11-28 05:07:17,053] A new study created in memory with name: no-name-c790f730-e715-4ed7-a526-b800cf3d607d\n[I 2025-11-28 05:07:27,562] Trial 0 finished with value: 0.7619088803894627 and parameters: {'max_depth': 3, 'learning_rate': 0.2339039476595304, 'n_estimators': 121, 'subsample': 0.7951176246010083, 'colsample_bytree': 0.9565841028848683, 'gamma': 3.900032048251767, 'reg_alpha': 2.1716365248049216, 'reg_lambda': 3.0218781895715665}. Best is trial 0 with value: 0.7619088803894627.\n[I 2025-11-28 05:08:07,000] Trial 1 finished with value: 0.7657016745992204 and parameters: {'max_depth': 10, 'learning_rate': 0.04564230328232159, 'n_estimators': 355, 'subsample': 0.5189118362716376, 'colsample_bytree': 0.7585659471346573, 'gamma': 2.5038573625988194, 'reg_alpha': 2.1580702547855934, 'reg_lambda': 1.6395407460369587}. Best is trial 1 with value: 0.7657016745992204.\n[I 2025-11-28 05:08:16,869] Trial 2 finished with value: 0.7581223355034685 and parameters: {'max_depth': 8, 'learning_rate': 0.10223346160046452, 'n_estimators': 72, 'subsample': 0.6404965906875151, 'colsample_bytree': 0.7818494097605908, 'gamma': 0.9821512255248366, 'reg_alpha': 0.5995963059989284, 'reg_lambda': 2.7246507670448032}. Best is trial 1 with value: 0.7657016745992204.\n[I 2025-11-28 05:08:46,601] Trial 3 finished with value: 0.7630178607546051 and parameters: {'max_depth': 4, 'learning_rate': 0.10691938844375237, 'n_estimators': 429, 'subsample': 0.7240683139719648, 'colsample_bytree': 0.9105681081380483, 'gamma': 4.063948586149545, 'reg_alpha': 1.2532964520690837, 'reg_lambda': 3.4973028956449124}. Best is trial 1 with value: 0.7657016745992204.\n[I 2025-11-28 05:09:10,552] Trial 4 finished with value: 0.7571597557497147 and parameters: {'max_depth': 10, 'learning_rate': 0.03612314220916706, 'n_estimators': 152, 'subsample': 0.5429947777418676, 'colsample_bytree': 0.7801614735301039, 'gamma': 0.22250184935399708, 'reg_alpha': 0.7546991752136428, 'reg_lambda': 1.290537723352373}. Best is trial 1 with value: 0.7657016745992204.\n[I 2025-11-28 05:09:25,751] Trial 5 finished with value: 0.7477451999990522 and parameters: {'max_depth': 10, 'learning_rate': 0.040629626489188594, 'n_estimators': 107, 'subsample': 0.9160904377237605, 'colsample_bytree': 0.987392692996973, 'gamma': 2.834688411474855, 'reg_alpha': 3.914403496138517, 'reg_lambda': 4.61280147652885}. Best is trial 1 with value: 0.7657016745992204.\n[I 2025-11-28 05:10:29,119] Trial 6 finished with value: 0.7657390899909208 and parameters: {'max_depth': 9, 'learning_rate': 0.04699921296503795, 'n_estimators': 305, 'subsample': 0.5282929863599877, 'colsample_bytree': 0.8687593301488974, 'gamma': 0.5062262071876195, 'reg_alpha': 2.210412993993587, 'reg_lambda': 1.4190431163176882}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:10:34,866] Trial 7 finished with value: 0.7383302404674308 and parameters: {'max_depth': 4, 'learning_rate': 0.13370866464033324, 'n_estimators': 59, 'subsample': 0.5527219546700405, 'colsample_bytree': 0.6856204595580865, 'gamma': 3.6642605360574225, 'reg_alpha': 2.911832573383415, 'reg_lambda': 0.5143720852476991}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:11:04,111] Trial 8 finished with value: 0.7651235118653663 and parameters: {'max_depth': 10, 'learning_rate': 0.20315434021347004, 'n_estimators': 118, 'subsample': 0.7489408577017662, 'colsample_bytree': 0.631957304043127, 'gamma': 0.23840613670939703, 'reg_alpha': 3.105750553412041, 'reg_lambda': 0.4467232400182869}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:11:10,500] Trial 9 finished with value: 0.7217684571851726 and parameters: {'max_depth': 5, 'learning_rate': 0.0764451583870585, 'n_estimators': 65, 'subsample': 0.9478878730373215, 'colsample_bytree': 0.7886800380066256, 'gamma': 0.7347794219554654, 'reg_alpha': 4.850342769630327, 'reg_lambda': 4.663556433822435}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:11:36,636] Trial 10 finished with value: 0.7289885023631755 and parameters: {'max_depth': 7, 'learning_rate': 0.015636057701728424, 'n_estimators': 254, 'subsample': 0.6508988351853779, 'colsample_bytree': 0.5236538757215777, 'gamma': 1.6080448220278294, 'reg_alpha': 1.6027524745779442, 'reg_lambda': 1.7986313460630754}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:12:21,521] Trial 11 finished with value: 0.756867490171552 and parameters: {'max_depth': 8, 'learning_rate': 0.01752599357497292, 'n_estimators': 366, 'subsample': 0.5009851342707876, 'colsample_bytree': 0.871150982650269, 'gamma': 2.2319551169458633, 'reg_alpha': 2.148713430521156, 'reg_lambda': 1.6309833946218888}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:12:57,016] Trial 12 finished with value: 0.7609137377427155 and parameters: {'max_depth': 9, 'learning_rate': 0.028841541547155167, 'n_estimators': 308, 'subsample': 0.6101960880003416, 'colsample_bytree': 0.8522238412483172, 'gamma': 4.964973595453927, 'reg_alpha': 0.014121015739295029, 'reg_lambda': 2.034171698663901}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:13:21,158] Trial 13 finished with value: 0.7645835353007289 and parameters: {'max_depth': 8, 'learning_rate': 0.058767220072469914, 'n_estimators': 228, 'subsample': 0.5814556264178762, 'colsample_bytree': 0.6892947382307494, 'gamma': 2.6000229713439484, 'reg_alpha': 3.5348883297445433, 'reg_lambda': 1.015639631379249}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:14:06,942] Trial 14 finished with value: 0.7607195759881653 and parameters: {'max_depth': 6, 'learning_rate': 0.02579176201696884, 'n_estimators': 442, 'subsample': 0.8348847860389799, 'colsample_bytree': 0.844086604000599, 'gamma': 1.5995170729559494, 'reg_alpha': 1.6864382428998443, 'reg_lambda': 2.3106320154771716}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:14:37,190] Trial 15 finished with value: 0.7645666410351921 and parameters: {'max_depth': 9, 'learning_rate': 0.05601004815175091, 'n_estimators': 347, 'subsample': 0.6900304296908284, 'colsample_bytree': 0.7148645505981474, 'gamma': 3.217906272452079, 'reg_alpha': 2.511061713404469, 'reg_lambda': 0.04888367954360784}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:15:35,862] Trial 16 finished with value: 0.7563871133961049 and parameters: {'max_depth': 9, 'learning_rate': 0.012598480749591773, 'n_estimators': 496, 'subsample': 0.5090855506181616, 'colsample_bytree': 0.5849377102944768, 'gamma': 1.8968670612294716, 'reg_alpha': 4.0257560377837525, 'reg_lambda': 3.8573449051020905}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:16:00,355] Trial 17 finished with value: 0.7397528069630512 and parameters: {'max_depth': 7, 'learning_rate': 0.02208981211900238, 'n_estimators': 208, 'subsample': 0.5949699229020523, 'colsample_bytree': 0.9380876317121567, 'gamma': 1.0601834693380963, 'reg_alpha': 2.6846913254549523, 'reg_lambda': 1.0157410786726402}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:16:34,484] Trial 18 finished with value: 0.7637145444062617 and parameters: {'max_depth': 9, 'learning_rate': 0.03929355932044046, 'n_estimators': 307, 'subsample': 0.6741290153610835, 'colsample_bytree': 0.8229152297899398, 'gamma': 3.1970280810267697, 'reg_alpha': 2.001531063356305, 'reg_lambda': 2.7527255034571843}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:17:08,987] Trial 19 finished with value: 0.7652326842800561 and parameters: {'max_depth': 6, 'learning_rate': 0.07512994151052874, 'n_estimators': 384, 'subsample': 0.8368772512984983, 'colsample_bytree': 0.7395595591177432, 'gamma': 1.4153145595076426, 'reg_alpha': 1.0889391377438857, 'reg_lambda': 1.435837145167992}. Best is trial 6 with value: 0.7657390899909208.\n[I 2025-11-28 05:17:47,698] Trial 20 finished with value: 0.7663684302068685 and parameters: {'max_depth': 10, 'learning_rate': 0.04780527775444687, 'n_estimators': 297, 'subsample': 0.5544958117220327, 'colsample_bytree': 0.8994883750563286, 'gamma': 2.2289682784562634, 'reg_alpha': 3.201431566376442, 'reg_lambda': 2.1548793072696113}. Best is trial 20 with value: 0.7663684302068685.\n[I 2025-11-28 05:18:30,555] Trial 21 finished with value: 0.765453284949384 and parameters: {'max_depth': 10, 'learning_rate': 0.04513050493985305, 'n_estimators': 301, 'subsample': 0.5542128927043671, 'colsample_bytree': 0.8964434332611384, 'gamma': 2.02462819112911, 'reg_alpha': 3.31723098486604, 'reg_lambda': 1.9907248355506557}. Best is trial 20 with value: 0.7663684302068685.\n[I 2025-11-28 05:18:54,714] Trial 22 finished with value: 0.7659222832264315 and parameters: {'max_depth': 10, 'learning_rate': 0.06889901860944508, 'n_estimators': 186, 'subsample': 0.5028514987287281, 'colsample_bytree': 0.9965188540384295, 'gamma': 2.43722762748744, 'reg_alpha': 3.7753684132688434, 'reg_lambda': 2.308537596134614}. Best is trial 20 with value: 0.7663684302068685.\n[I 2025-11-28 05:19:12,993] Trial 23 finished with value: 0.7631569061506712 and parameters: {'max_depth': 9, 'learning_rate': 0.07545258431394063, 'n_estimators': 185, 'subsample': 0.6091499355800511, 'colsample_bytree': 0.9727940143992498, 'gamma': 4.626883801260858, 'reg_alpha': 4.488101649852952, 'reg_lambda': 2.3576209991037107}. Best is trial 20 with value: 0.7663684302068685.\n[I 2025-11-28 05:19:34,316] Trial 24 finished with value: 0.7656387539337697 and parameters: {'max_depth': 8, 'learning_rate': 0.14164657460228794, 'n_estimators': 265, 'subsample': 0.5661416970482548, 'colsample_bytree': 0.9203394388515853, 'gamma': 3.0223511595114485, 'reg_alpha': 3.6391959469704585, 'reg_lambda': 3.3034783154553486}. Best is trial 20 with value: 0.7663684302068685.\n[I 2025-11-28 05:20:13,065] Trial 25 finished with value: 0.7660408192978954 and parameters: {'max_depth': 10, 'learning_rate': 0.061737002395142515, 'n_estimators': 172, 'subsample': 0.5051469963353508, 'colsample_bytree': 0.9993926903612812, 'gamma': 0.6491806655171874, 'reg_alpha': 4.470365411190396, 'reg_lambda': 2.260593852110417}. Best is trial 20 with value: 0.7663684302068685.\n[I 2025-11-28 05:20:47,130] Trial 26 finished with value: 0.7668096964845319 and parameters: {'max_depth': 10, 'learning_rate': 0.06632917141639863, 'n_estimators': 171, 'subsample': 0.6199900342765811, 'colsample_bytree': 0.9960998096623755, 'gamma': 1.106326707843751, 'reg_alpha': 4.305360578445439, 'reg_lambda': 3.778990982649021}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:21:27,588] Trial 27 finished with value: 0.7630767299660535 and parameters: {'max_depth': 10, 'learning_rate': 0.031173398050403762, 'n_estimators': 240, 'subsample': 0.6346898101061922, 'colsample_bytree': 0.999540575923499, 'gamma': 1.01646609057435, 'reg_alpha': 4.33763978481205, 'reg_lambda': 4.047665787261554}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:21:57,462] Trial 28 finished with value: 0.7651496874959918 and parameters: {'max_depth': 8, 'learning_rate': 0.10051305187576412, 'n_estimators': 163, 'subsample': 0.6951823810513224, 'colsample_bytree': 0.9435855057154134, 'gamma': 0.03564718740643191, 'reg_alpha': 4.892437772521861, 'reg_lambda': 4.966405900871739}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:22:09,664] Trial 29 finished with value: 0.763997942461941 and parameters: {'max_depth': 3, 'learning_rate': 0.224974398934888, 'n_estimators': 152, 'subsample': 0.7977497007067814, 'colsample_bytree': 0.9557607831375237, 'gamma': 1.4212308457523661, 'reg_alpha': 4.356248516979121, 'reg_lambda': 2.8778167269154475}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:22:44,373] Trial 30 finished with value: 0.7626209683612474 and parameters: {'max_depth': 7, 'learning_rate': 0.28333252498408346, 'n_estimators': 211, 'subsample': 0.5822743513793973, 'colsample_bytree': 0.8977838348044626, 'gamma': 0.6073494369181787, 'reg_alpha': 4.605074509445137, 'reg_lambda': 3.415691221619304}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:23:11,196] Trial 31 finished with value: 0.7653330903571696 and parameters: {'max_depth': 10, 'learning_rate': 0.06493309735104358, 'n_estimators': 184, 'subsample': 0.533198460229831, 'colsample_bytree': 0.9970961613516959, 'gamma': 2.114243478322987, 'reg_alpha': 3.8901910047260198, 'reg_lambda': 2.4436272888866273}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:23:29,322] Trial 32 finished with value: 0.7658878061123924 and parameters: {'max_depth': 10, 'learning_rate': 0.08875878929866873, 'n_estimators': 133, 'subsample': 0.50246648315657, 'colsample_bytree': 0.9653822332605443, 'gamma': 2.467007885120961, 'reg_alpha': 3.4841177740568416, 'reg_lambda': 3.110168422403472}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:23:45,917] Trial 33 finished with value: 0.7646794634384296 and parameters: {'max_depth': 9, 'learning_rate': 0.13706034431204517, 'n_estimators': 94, 'subsample': 0.616904239922734, 'colsample_bytree': 0.9366413035805725, 'gamma': 1.2150827419848937, 'reg_alpha': 4.2086779326727175, 'reg_lambda': 2.1584812342743382}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:24:17,253] Trial 34 finished with value: 0.7646622572196777 and parameters: {'max_depth': 10, 'learning_rate': 0.05234001465221979, 'n_estimators': 188, 'subsample': 0.5674667411922488, 'colsample_bytree': 0.9671325480345164, 'gamma': 1.8023425037981444, 'reg_alpha': 3.1330554149241294, 'reg_lambda': 2.5678136754166294}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:24:43,104] Trial 35 finished with value: 0.765193784403257 and parameters: {'max_depth': 10, 'learning_rate': 0.06507827934200301, 'n_estimators': 281, 'subsample': 0.5340933629368022, 'colsample_bytree': 0.9229546028959623, 'gamma': 3.547388737835269, 'reg_alpha': 3.6976953069075105, 'reg_lambda': 4.013997381257174}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:25:04,755] Trial 36 finished with value: 0.7657610663641801 and parameters: {'max_depth': 9, 'learning_rate': 0.08765056034313605, 'n_estimators': 212, 'subsample': 0.6409054001549409, 'colsample_bytree': 0.8972768465929634, 'gamma': 2.4355110029215665, 'reg_alpha': 4.667294888980789, 'reg_lambda': 3.7147848655936757}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:25:18,231] Trial 37 finished with value: 0.740567824963924 and parameters: {'max_depth': 10, 'learning_rate': 0.0344348640306681, 'n_estimators': 90, 'subsample': 0.5302628806189582, 'colsample_bytree': 0.9782119297122857, 'gamma': 0.7891883196872544, 'reg_alpha': 4.061362984299017, 'reg_lambda': 3.1725454379525946}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:25:31,401] Trial 38 finished with value: 0.7638889737212097 and parameters: {'max_depth': 9, 'learning_rate': 0.11783057050694033, 'n_estimators': 136, 'subsample': 0.5575842361815931, 'colsample_bytree': 0.9996981572330311, 'gamma': 4.015858015033994, 'reg_alpha': 4.979580719485841, 'reg_lambda': 2.901632490591746}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:26:04,595] Trial 39 finished with value: 0.7637203601439422 and parameters: {'max_depth': 10, 'learning_rate': 0.04725191803792191, 'n_estimators': 165, 'subsample': 0.5213548830809832, 'colsample_bytree': 0.9527075084415927, 'gamma': 0.3887967424857507, 'reg_alpha': 2.9295376422956934, 'reg_lambda': 2.5670397716174294}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:26:25,952] Trial 40 finished with value: 0.7659439514098785 and parameters: {'max_depth': 10, 'learning_rate': 0.18100576657017337, 'n_estimators': 278, 'subsample': 0.5843714826846909, 'colsample_bytree': 0.87095247581472, 'gamma': 2.830363156545078, 'reg_alpha': 3.8323216656573442, 'reg_lambda': 1.7491000600460826}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:26:47,529] Trial 41 finished with value: 0.7655409665249416 and parameters: {'max_depth': 10, 'learning_rate': 0.20102699494142984, 'n_estimators': 280, 'subsample': 0.5893347898231893, 'colsample_bytree': 0.8253402273862024, 'gamma': 2.704471881522652, 'reg_alpha': 3.8271561393813105, 'reg_lambda': 1.7881962065155208}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:27:11,759] Trial 42 finished with value: 0.7660610333891199 and parameters: {'max_depth': 10, 'learning_rate': 0.173492402217987, 'n_estimators': 326, 'subsample': 0.5480328001032361, 'colsample_bytree': 0.8029042994009634, 'gamma': 2.8487025021743415, 'reg_alpha': 3.3120706380543807, 'reg_lambda': 1.0516611694287779}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:27:35,926] Trial 43 finished with value: 0.7658668165859295 and parameters: {'max_depth': 9, 'learning_rate': 0.16362797184710817, 'n_estimators': 336, 'subsample': 0.6662510818826677, 'colsample_bytree': 0.8102431584024189, 'gamma': 2.8451233006315086, 'reg_alpha': 3.415800222840156, 'reg_lambda': 0.9929332320975123}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:27:58,883] Trial 44 finished with value: 0.7640692155734266 and parameters: {'max_depth': 10, 'learning_rate': 0.18054835013493628, 'n_estimators': 332, 'subsample': 0.7189042743971354, 'colsample_bytree': 0.8639093281737784, 'gamma': 3.5791534472183315, 'reg_alpha': 4.13478825718954, 'reg_lambda': 1.2694442693143695}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:28:26,623] Trial 45 finished with value: 0.7656632163074102 and parameters: {'max_depth': 9, 'learning_rate': 0.1212943726214425, 'n_estimators': 376, 'subsample': 0.6226070422083572, 'colsample_bytree': 0.776476020012781, 'gamma': 3.134950898584977, 'reg_alpha': 3.2166550505065814, 'reg_lambda': 0.7457201864964489}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:28:43,371] Trial 46 finished with value: 0.7620922382622576 and parameters: {'max_depth': 10, 'learning_rate': 0.24551497523152047, 'n_estimators': 255, 'subsample': 0.9910586273698336, 'colsample_bytree': 0.8823604990032401, 'gamma': 3.408862037652875, 'reg_alpha': 4.682327512157032, 'reg_lambda': 1.623381184695382}. Best is trial 26 with value: 0.7668096964845319.\n[I 2025-11-28 05:29:20,128] Trial 47 finished with value: 0.7670427552297412 and parameters: {'max_depth': 8, 'learning_rate': 0.09502306779774816, 'n_estimators': 407, 'subsample': 0.5441296871694551, 'colsample_bytree': 0.8023489318611722, 'gamma': 2.236678251235929, 'reg_alpha': 2.8940272572867385, 'reg_lambda': 1.8295240052755743}. Best is trial 47 with value: 0.7670427552297412.\n[I 2025-11-28 05:29:55,701] Trial 48 finished with value: 0.7666069317994268 and parameters: {'max_depth': 8, 'learning_rate': 0.09578733183373457, 'n_estimators': 416, 'subsample': 0.5589003783286073, 'colsample_bytree': 0.7412970997406751, 'gamma': 2.2973939885934658, 'reg_alpha': 2.616304274120746, 'reg_lambda': 1.9678690112526007}. Best is trial 47 with value: 0.7670427552297412.\n[I 2025-11-28 05:30:32,241] Trial 49 finished with value: 0.7656021743068884 and parameters: {'max_depth': 8, 'learning_rate': 0.0987920256642153, 'n_estimators': 422, 'subsample': 0.5441380240286938, 'colsample_bytree': 0.7386479085154114, 'gamma': 2.2416158990630217, 'reg_alpha': 2.785546610501214, 'reg_lambda': 1.9901808005754449}. Best is trial 47 with value: 0.7670427552297412.\n","output_type":"stream"},{"name":"stdout","text":"Best trial:\nF2 Macro: 0.7670427552297412\nBest hyperparameters: {'max_depth': 8, 'learning_rate': 0.09502306779774816, 'n_estimators': 407, 'subsample': 0.5441296871694551, 'colsample_bytree': 0.8023489318611722, 'gamma': 2.236678251235929, 'reg_alpha': 2.8940272572867385, 'reg_lambda': 1.8295240052755743}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"best_params = study.best_params.copy()\nbest_params.update({\n    \"objective\": \"binary:logistic\",\n    \"use_label_encoder\": False,\n    \"eval_metric\": \"logloss\",\n    \"random_state\": 42,\n    \"n_jobs\": -1\n})\n\nf2_scorer = make_scorer(fbeta_score, beta = 2, average = \"binary\")\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = 0.2, stratify = y, random_state = 42\n)\ncv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\nmodel = XGBClassifier(**best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:31:55.457594Z","iopub.execute_input":"2025-11-28T19:31:55.458039Z","iopub.status.idle":"2025-11-28T19:31:55.514151Z","shell.execute_reply.started":"2025-11-28T19:31:55.457843Z","shell.execute_reply":"2025-11-28T19:31:55.513034Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"cv_scores = cross_val_score(model, X_train, y_train, cv = cv, scoring = f2_scorer, n_jobs = -1)\n\nprint(\"CV F2 scores (train folds):\", np.round(cv_scores, 4))\nprint(\"CV F2 mean:\", np.round(cv_scores.mean(), 4))\nprint(\"CV F2 std:\", np.round(cv_scores.std(), 4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:31:55.516440Z","iopub.execute_input":"2025-11-28T19:31:55.517040Z","iopub.status.idle":"2025-11-28T19:32:24.315407Z","shell.execute_reply.started":"2025-11-28T19:31:55.517007Z","shell.execute_reply":"2025-11-28T19:32:24.314471Z"}},"outputs":[{"name":"stdout","text":"CV F2 scores (train folds): [0.6483 0.6485 0.6497 0.6507 0.652 ]\nCV F2 mean: 0.6498\nCV F2 std: 0.0014\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"model.fit(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:33:05.025030Z","iopub.execute_input":"2025-11-28T19:33:05.025353Z","iopub.status.idle":"2025-11-28T19:33:11.922448Z","shell.execute_reply.started":"2025-11-28T19:33:05.025331Z","shell.execute_reply":"2025-11-28T19:33:11.921526Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8023489318611722, device=None,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric='logloss', feature_types=None,\n              gamma=2.236678251235929, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.09502306779774816,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=8, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=407, n_jobs=-1,\n              num_parallel_tree=None, random_state=42, ...)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8023489318611722, device=None,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=&#x27;logloss&#x27;, feature_types=None,\n              gamma=2.236678251235929, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.09502306779774816,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=8, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=407, n_jobs=-1,\n              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=0.8023489318611722, device=None,\n              early_stopping_rounds=None, enable_categorical=False,\n              eval_metric=&#x27;logloss&#x27;, feature_types=None,\n              gamma=2.236678251235929, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=0.09502306779774816,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=8, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=407, n_jobs=-1,\n              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"inference = pd.read_csv(\"/kaggle/input/cpe342-karena/public_dataset/task1/test.csv\")\n\ninference.drop(columns = [\"id\", \"player_id\"], inplace = True)\ninference.drop(columns = weak_feature, inplace = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:33:11.923852Z","iopub.execute_input":"2025-11-28T19:33:11.924763Z","iopub.status.idle":"2025-11-28T19:33:12.205948Z","shell.execute_reply.started":"2025-11-28T19:33:11.924734Z","shell.execute_reply":"2025-11-28T19:33:12.205043Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"y_pred = model.predict(inference)\nsubmit = pd.read_csv(\"/kaggle/input/cpe342-karena/public_dataset/sample_submission.csv\")\nsubmit[\"task1\"] = y_pred\nsubmit = pd.read_csv(\"/kaggle/input/cpe342-karena/public_dataset/sample_submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:33:12.206823Z","iopub.execute_input":"2025-11-28T19:33:12.207159Z","iopub.status.idle":"2025-11-28T19:33:12.481010Z","shell.execute_reply.started":"2025-11-28T19:33:12.207133Z","shell.execute_reply":"2025-11-28T19:33:12.480069Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# StackEnsemble","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/cpe342-karena/public_dataset/task1/train.csv\")\n\ndf.drop(columns = [\"id\", \"player_id\"], inplace = True)\ndf = df.dropna(subset = [\"is_cheater\"])\n\nX = df.drop(columns = [\"is_cheater\"])\ny = df[\"is_cheater\"].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:33:12.483160Z","iopub.execute_input":"2025-11-28T19:33:12.483888Z","iopub.status.idle":"2025-11-28T19:33:13.218958Z","shell.execute_reply.started":"2025-11-28T19:33:12.483858Z","shell.execute_reply":"2025-11-28T19:33:13.217698Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb_for_fi = XGBClassifier(\n    objective = \"binary:logistic\",\n    eval_metric = \"logloss\",\n    use_label_encoder = False,\n    random_state = 42,\n    n_jobs = 1\n)\n\nxgb_for_fi.fit(X, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:33:13.219841Z","iopub.execute_input":"2025-11-28T19:33:13.220105Z","iopub.status.idle":"2025-11-28T19:33:16.934167Z","shell.execute_reply.started":"2025-11-28T19:33:13.220086Z","shell.execute_reply":"2025-11-28T19:33:16.933186Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric='logloss',\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=1, num_parallel_tree=None, random_state=42, ...)","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=1, num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=&#x27;logloss&#x27;,\n              feature_types=None, gamma=None, grow_policy=None,\n              importance_type=None, interaction_constraints=None,\n              learning_rate=None, max_bin=None, max_cat_threshold=None,\n              max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n              max_leaves=None, min_child_weight=None, missing=nan,\n              monotone_constraints=None, multi_strategy=None, n_estimators=None,\n              n_jobs=1, num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# Extract Feature Importance\nbooster = xgb_for_fi.get_booster()\nfi = booster.get_score(importance_type = \"gain\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfi_named = pd.DataFrame(fi.items(), columns = [\"feature\", \"importance\"])\nTop15 = fi_named.sort_values(\"importance\", ascending = False).head(15)\n\nsns.barplot(data = Top15, y = \"feature\", x = \"importance\", orient = \"h\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:33:16.935214Z","iopub.execute_input":"2025-11-28T19:33:16.935534Z","iopub.status.idle":"2025-11-28T19:33:17.165738Z","shell.execute_reply.started":"2025-11-28T19:33:16.935507Z","shell.execute_reply":"2025-11-28T19:33:17.164649Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"fi_named_sorted = fi_named.sort_values(\"importance\", ascending = False)\nfi_named_sorted[\"cumulative_importance\"] = fi_named_sorted[\"importance\"].cumsum() / fi_named_sorted[\"importance\"].sum()\nselected_features = fi_named_sorted[fi_named_sorted[\"cumulative_importance\"] <= 0.95][\"feature\"]\n\nprint(f\"Selected {len(selected_features)} features out of {len(fi_named)} total features\")\nX_selected = X[selected_features].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:33:17.168059Z","iopub.execute_input":"2025-11-28T19:33:17.168614Z","iopub.status.idle":"2025-11-28T19:33:17.185830Z","shell.execute_reply.started":"2025-11-28T19:33:17.168588Z","shell.execute_reply":"2025-11-28T19:33:17.184736Z"}},"outputs":[{"name":"stdout","text":"Selected 24 features out of 31 total features\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.metrics import fbeta_score, make_scorer\n\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nestimators = [\n    (\"XGBoost\", XGBClassifier(\n        objective = \"binary:logistic\",\n        eval_metric = \"logloss\",\n        use_label_encoder = False,\n        random_state = 42,\n        n_jobs = 1\n    )),\n    (\"LightGBM\", LGBMClassifier(\n        objective = \"binary\",\n        metric = \"binary_logloss\",\n        random_state = 42,\n        n_jobs = 1\n    )),\n    (\"CatBoost\", CatBoostClassifier(\n        loss_function = \"Logloss\",\n        eval_metric = \"Logloss\",\n        verbose = 0,\n        random_state = 42,\n        thread_count = 1\n    ))\n]\n\nblender = LogisticRegression(solver = \"saga\", max_iter = 2000, random_state = 42)\n\nstack = StackingClassifier(\n    estimators = estimators,\n    final_estimator = blender,\n    stack_method = \"predict_proba\",\n    cv = 5,\n    n_jobs = -1,\n    passthrough = False\n)\n\ncv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\nfbeta_scorer = make_scorer(fbeta_score, beta = 2, average = \"binary\")\nscores = cross_val_score(stack, X_selected, y, cv = cv, scoring = fbeta_scorer, n_jobs = -1)\nprint(f\"Stacking (XGB+LGBM+Cat -> LR): mean F2 = {scores.mean():.4f}  (+/- {scores.std():.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T19:33:17.186819Z","iopub.execute_input":"2025-11-28T19:33:17.187137Z","iopub.status.idle":"2025-11-28T19:42:25.737860Z","shell.execute_reply.started":"2025-11-28T19:33:17.187099Z","shell.execute_reply":"2025-11-28T19:42:25.736285Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.121103 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035952 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040721 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033326 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063522 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044485 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.088493 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094850 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063825 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033511 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034884 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101045 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028533 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094406 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097339 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033250 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098005 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093851 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099193 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.085476 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21844, number of negative: 40716\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.093089 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098383 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62560, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349169 -> initscore=-0.622695\n[LightGBM] [Info] Start training from score -0.622695\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.090152 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051207 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019621 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349158 -> initscore=-0.622741\n[LightGBM] [Info] Start training from score -0.622741\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40717\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018147 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349142 -> initscore=-0.622811\n[LightGBM] [Info] Start training from score -0.622811\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40717\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018364 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349142 -> initscore=-0.622811\n[LightGBM] [Info] Start training from score -0.622811\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40717\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018273 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349142 -> initscore=-0.622811\n[LightGBM] [Info] Start training from score -0.622811\n[LightGBM] [Info] Number of positive: 21843, number of negative: 40717\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032185 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62560, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349153 -> initscore=-0.622765\n[LightGBM] [Info] Start training from score -0.622765\nStacking (XGB+LGBM+Cat -> LR): mean F2 = 0.6576  (+/- 0.0048)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## Hyperparameter","metadata":{}},{"cell_type":"code","source":"import optuna\n\ndef objective_xgb(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.03, 0.3, log = True),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 10.0, log = True),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log = True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log = True),\n\n        \"objective\": \"binary:logistic\",\n        \"use_label_encoder\": False,\n        \"random_state\": 42,\n        \"n_jobs\": -1\n    }\n\n    model = XGBClassifier(**params)\n    scores = cross_val_score(model, X_selected, y, cv = cv, scoring = fbeta_scorer, n_jobs = -1)\n    return 1.0 - scores.mean()\n\nstudy_xgb = optuna.create_study(direction = \"minimize\", sampler = optuna.samplers.TPESampler(seed = 42))\nstudy_xgb.optimize(objective_xgb, n_trials = 100)\nprint(\"Best XGB params:\", study_xgb.best_params, \"best f2:\", 1 - study_xgb.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T05:58:41.863664Z","iopub.execute_input":"2025-11-28T05:58:41.865543Z","iopub.status.idle":"2025-11-28T06:46:58.325975Z","shell.execute_reply.started":"2025-11-28T05:58:41.865498Z","shell.execute_reply":"2025-11-28T06:46:58.324941Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"[I 2025-11-28 05:58:41,878] A new study created in memory with name: no-name-51519c00-951a-4ef4-be68-8a2decb926f3\n[I 2025-11-28 05:59:10,139] Trial 0 finished with value: 0.35592045924251203 and parameters: {'n_estimators': 218, 'max_depth': 10, 'learning_rate': 0.1618509290001068, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.4936111842654619, 'gamma': 2.5348407664333426e-07, 'reg_alpha': 3.3323645788192616e-08, 'reg_lambda': 0.6245760287469893}. Best is trial 0 with value: 0.35592045924251203.\n[I 2025-11-28 05:59:42,557] Trial 1 finished with value: 0.37820963315607636 and parameters: {'n_estimators': 321, 'max_depth': 8, 'learning_rate': 0.031456163175583855, 'subsample': 0.9849549260809971, 'colsample_bytree': 0.899465584480253, 'gamma': 8.148018307012941e-07, 'reg_alpha': 4.329370014459266e-07, 'reg_lambda': 4.4734294104626844e-07}. Best is trial 0 with value: 0.35592045924251203.\n[I 2025-11-28 05:59:57,609] Trial 2 finished with value: 0.3611971459471951 and parameters: {'n_estimators': 187, 'max_depth': 7, 'learning_rate': 0.08110848199986004, 'subsample': 0.645614570099021, 'colsample_bytree': 0.7671117368334277, 'gamma': 1.8007140198129195e-07, 'reg_alpha': 4.258943089524393e-06, 'reg_lambda': 1.9826980964985924e-05}. Best is trial 0 with value: 0.35592045924251203.\n[I 2025-11-28 06:00:25,779] Trial 3 finished with value: 0.3630179592416025 and parameters: {'n_estimators': 255, 'max_depth': 9, 'learning_rate': 0.04751109467735623, 'subsample': 0.7571172192068059, 'colsample_bytree': 0.7554487413172255, 'gamma': 2.6185068507773707e-08, 'reg_alpha': 0.0029369981104377003, 'reg_lambda': 3.425445902633376e-07}. Best is trial 0 with value: 0.35592045924251203.\n[I 2025-11-28 06:00:36,696] Trial 4 finished with value: 0.352903666125046 and parameters: {'n_estimators': 79, 'max_depth': 10, 'learning_rate': 0.27717450958881745, 'subsample': 0.9041986740582306, 'colsample_bytree': 0.5827682615040224, 'gamma': 7.569183361880229e-08, 'reg_alpha': 0.014391207615728067, 'reg_lambda': 9.148975058772307e-05}. Best is trial 4 with value: 0.352903666125046.\n[I 2025-11-28 06:00:42,971] Trial 5 finished with value: 0.5035119918528538 and parameters: {'n_estimators': 105, 'max_depth': 6, 'learning_rate': 0.03247205514450286, 'subsample': 0.954660201039391, 'colsample_bytree': 0.5552679889600102, 'gamma': 0.009176996354542699, 'reg_alpha': 6.388511557344611e-06, 'reg_lambda': 0.0004793052550782129}. Best is trial 4 with value: 0.352903666125046.\n[I 2025-11-28 06:00:54,400] Trial 6 finished with value: 0.35523854518235143 and parameters: {'n_estimators': 296, 'max_depth': 4, 'learning_rate': 0.27970864055344435, 'subsample': 0.8875664116805573, 'colsample_bytree': 0.9636993649385135, 'gamma': 1.1309571585271483, 'reg_alpha': 0.002404915432737351, 'reg_lambda': 1.9809253750493907}. Best is trial 4 with value: 0.352903666125046.\n[I 2025-11-28 06:00:58,372] Trial 7 finished with value: 0.6509641711924974 and parameters: {'n_estimators': 89, 'max_depth': 4, 'learning_rate': 0.033292663683309284, 'subsample': 0.6626651653816322, 'colsample_bytree': 0.6332063738136893, 'gamma': 2.7678419414850017e-06, 'reg_alpha': 0.28749982347407854, 'reg_lambda': 1.6247252885719427e-05}. Best is trial 4 with value: 0.352903666125046.\n[I 2025-11-28 06:01:06,677] Trial 8 finished with value: 0.4025164963219331 and parameters: {'n_estimators': 176, 'max_depth': 7, 'learning_rate': 0.041499749925659864, 'subsample': 0.9010984903770198, 'colsample_bytree': 0.44473038620786254, 'gamma': 7.620481786158549, 'reg_alpha': 0.08916674715636537, 'reg_lambda': 6.143857495033091e-07}. Best is trial 4 with value: 0.352903666125046.\n[I 2025-11-28 06:01:14,248] Trial 9 finished with value: 0.376845569993986 and parameters: {'n_estimators': 52, 'max_depth': 9, 'learning_rate': 0.15274907837455656, 'subsample': 0.8645035840204937, 'colsample_bytree': 0.8627622080115674, 'gamma': 4.638759594322625e-08, 'reg_alpha': 1.683416412018213e-05, 'reg_lambda': 1.1036250149900698e-07}. Best is trial 4 with value: 0.352903666125046.\n[I 2025-11-28 06:02:32,790] Trial 10 finished with value: 0.3495184077767566 and parameters: {'n_estimators': 476, 'max_depth': 10, 'learning_rate': 0.27967803877874403, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.6383254458186773, 'gamma': 0.00012792286110097315, 'reg_alpha': 4.3444691085504115, 'reg_lambda': 0.012278672550406745}. Best is trial 10 with value: 0.3495184077767566.\n[I 2025-11-28 06:03:43,733] Trial 11 finished with value: 0.34557442986187814 and parameters: {'n_estimators': 477, 'max_depth': 10, 'learning_rate': 0.29697520068240946, 'subsample': 0.5085930751344795, 'colsample_bytree': 0.6478041190213072, 'gamma': 8.113466471626518e-05, 'reg_alpha': 7.906845974305311, 'reg_lambda': 0.00982411342838796}. Best is trial 11 with value: 0.34557442986187814.\n[I 2025-11-28 06:05:03,041] Trial 12 finished with value: 0.3490942506869722 and parameters: {'n_estimators': 498, 'max_depth': 10, 'learning_rate': 0.19933580085944358, 'subsample': 0.5018203726540555, 'colsample_bytree': 0.687691175631377, 'gamma': 0.00023628205952709374, 'reg_alpha': 9.42273503424589, 'reg_lambda': 0.01659099034836456}. Best is trial 11 with value: 0.34557442986187814.\n[I 2025-11-28 06:06:02,672] Trial 13 finished with value: 0.34471671938282555 and parameters: {'n_estimators': 497, 'max_depth': 8, 'learning_rate': 0.1774397179535628, 'subsample': 0.5048579886309473, 'colsample_bytree': 0.7085337539039134, 'gamma': 0.00021407397553555843, 'reg_alpha': 9.868745464853815, 'reg_lambda': 0.029934154983106365}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:06:50,225] Trial 14 finished with value: 0.350628869348221 and parameters: {'n_estimators': 402, 'max_depth': 8, 'learning_rate': 0.10916645275328549, 'subsample': 0.5797654491534234, 'colsample_bytree': 0.7678321015144071, 'gamma': 0.0078123578955274315, 'reg_alpha': 0.6113319020885644, 'reg_lambda': 0.04394253214905646}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:07:18,166] Trial 15 finished with value: 0.34886534655131807 and parameters: {'n_estimators': 412, 'max_depth': 6, 'learning_rate': 0.19514521852025354, 'subsample': 0.592847000975673, 'colsample_bytree': 0.8344376716296752, 'gamma': 9.73691574493519e-06, 'reg_alpha': 0.00012997359677011742, 'reg_lambda': 0.0016838781054430317}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:08:08,401] Trial 16 finished with value: 0.34975687527075294 and parameters: {'n_estimators': 415, 'max_depth': 8, 'learning_rate': 0.11220521394846435, 'subsample': 0.5646332716363499, 'colsample_bytree': 0.7134379941114688, 'gamma': 0.0060430822493950116, 'reg_alpha': 1.4148523971874911, 'reg_lambda': 0.2818936773599525}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:08:38,188] Trial 17 finished with value: 0.35679847374261275 and parameters: {'n_estimators': 352, 'max_depth': 9, 'learning_rate': 0.07033257128287618, 'subsample': 0.692899837707436, 'colsample_bytree': 0.40425860244424616, 'gamma': 3.154443500139822e-05, 'reg_alpha': 0.04026246621022618, 'reg_lambda': 5.3669561170728075}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:09:00,257] Trial 18 finished with value: 0.34570435062121674 and parameters: {'n_estimators': 458, 'max_depth': 5, 'learning_rate': 0.21312708298541708, 'subsample': 0.5514779294474169, 'colsample_bytree': 0.5422409619433222, 'gamma': 0.0016004281583744105, 'reg_alpha': 9.86380182335543, 'reg_lambda': 0.002802882974474291}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:09:10,976] Trial 19 finished with value: 0.3595173659928331 and parameters: {'n_estimators': 379, 'max_depth': 3, 'learning_rate': 0.1495216764258819, 'subsample': 0.6463144155801708, 'colsample_bytree': 0.6640777558857779, 'gamma': 0.12239159247433272, 'reg_alpha': 0.0007576681230466948, 'reg_lambda': 0.10390518293027717}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:10:09,851] Trial 20 finished with value: 0.35107496416879935 and parameters: {'n_estimators': 446, 'max_depth': 9, 'learning_rate': 0.22369686937260133, 'subsample': 0.7137198757921601, 'colsample_bytree': 0.591846342851923, 'gamma': 3.0058706037516875e-05, 'reg_alpha': 0.26616384234482043, 'reg_lambda': 0.0049393924646631724}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:10:31,263] Trial 21 finished with value: 0.34681423214511775 and parameters: {'n_estimators': 458, 'max_depth': 5, 'learning_rate': 0.22279679746427333, 'subsample': 0.5424482178411704, 'colsample_bytree': 0.5243652301935524, 'gamma': 0.0012094101880661978, 'reg_alpha': 9.84237291748935, 'reg_lambda': 0.0011862256866099806}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:10:52,193] Trial 22 finished with value: 0.34932062804454367 and parameters: {'n_estimators': 494, 'max_depth': 5, 'learning_rate': 0.12845878859289742, 'subsample': 0.5975118290673396, 'colsample_bytree': 0.498004624521996, 'gamma': 0.0009737644340935406, 'reg_alpha': 2.491498931567676, 'reg_lambda': 0.00012519687720739223}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:11:12,679] Trial 23 finished with value: 0.3460139475129441 and parameters: {'n_estimators': 442, 'max_depth': 5, 'learning_rate': 0.1824691616445026, 'subsample': 0.5266713465812178, 'colsample_bytree': 0.6163055314059155, 'gamma': 0.0631085635095782, 'reg_alpha': 0.7634167976549896, 'reg_lambda': 0.004266317647146156}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:11:44,475] Trial 24 finished with value: 0.34944382808280106 and parameters: {'n_estimators': 361, 'max_depth': 7, 'learning_rate': 0.23163462534742654, 'subsample': 0.6102236446110821, 'colsample_bytree': 0.7176271385293098, 'gamma': 0.0013463230480902238, 'reg_alpha': 0.024192468217751482, 'reg_lambda': 0.0678233261718571}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:12:10,797] Trial 25 finished with value: 0.3496321185319542 and parameters: {'n_estimators': 446, 'max_depth': 6, 'learning_rate': 0.29847828931173387, 'subsample': 0.5474775686023864, 'colsample_bytree': 0.580726118576525, 'gamma': 9.222628900288715e-05, 'reg_alpha': 0.1441089293295837, 'reg_lambda': 1.4984260363555197e-08}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:13:09,870] Trial 26 finished with value: 0.3470795186295883 and parameters: {'n_estimators': 471, 'max_depth': 8, 'learning_rate': 0.24316019225973548, 'subsample': 0.5048925598856114, 'colsample_bytree': 0.66552986351982, 'gamma': 7.120875402030538e-06, 'reg_alpha': 1.889860699071598, 'reg_lambda': 0.4458322026467359}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:13:22,311] Trial 27 finished with value: 0.3529870888546537 and parameters: {'n_estimators': 326, 'max_depth': 4, 'learning_rate': 0.17641435854845613, 'subsample': 0.6164473667902702, 'colsample_bytree': 0.8058436404426828, 'gamma': 0.04041545789379561, 'reg_alpha': 0.00794407258026495, 'reg_lambda': 1.0875350813198349e-05}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:13:33,521] Trial 28 finished with value: 0.36274164137353826 and parameters: {'n_estimators': 397, 'max_depth': 3, 'learning_rate': 0.13436372843830888, 'subsample': 0.5597404566553518, 'colsample_bytree': 0.536190864258645, 'gamma': 0.00042479335221734505, 'reg_alpha': 3.1069277755442704, 'reg_lambda': 0.00039305920822029703}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:14:29,034] Trial 29 finished with value: 0.35583965306742293 and parameters: {'n_estimators': 433, 'max_depth': 10, 'learning_rate': 0.08685931062583865, 'subsample': 0.8198527594338868, 'colsample_bytree': 0.471645973649337, 'gamma': 1.6428724619203273e-06, 'reg_alpha': 3.0439381964747037e-07, 'reg_lambda': 1.1712007210444573}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:14:46,531] Trial 30 finished with value: 0.3612862110532399 and parameters: {'n_estimators': 252, 'max_depth': 7, 'learning_rate': 0.06869612210277144, 'subsample': 0.7697702110416265, 'colsample_bytree': 0.7193069827634617, 'gamma': 1.9925138207574393e-05, 'reg_alpha': 2.4249713988677598e-08, 'reg_lambda': 0.01918964345123732}. Best is trial 13 with value: 0.34471671938282555.\n[I 2025-11-28 06:15:09,654] Trial 31 finished with value: 0.34266024463931 and parameters: {'n_estimators': 499, 'max_depth': 5, 'learning_rate': 0.18388501273307045, 'subsample': 0.5245398217695244, 'colsample_bytree': 0.6280838727536953, 'gamma': 0.2106905758587501, 'reg_alpha': 0.6264723877782022, 'reg_lambda': 0.0042684493278705455}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:15:33,570] Trial 32 finished with value: 0.34780657179619 and parameters: {'n_estimators': 498, 'max_depth': 5, 'learning_rate': 0.17046146031006568, 'subsample': 0.5478988296047008, 'colsample_bytree': 0.6162331501090953, 'gamma': 0.5336266936847212, 'reg_alpha': 8.920408547279312, 'reg_lambda': 0.004333223247991297}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:16:02,639] Trial 33 finished with value: 0.347968007710623 and parameters: {'n_estimators': 464, 'max_depth': 6, 'learning_rate': 0.250642241703703, 'subsample': 0.5279991367610904, 'colsample_bytree': 0.6603707113094738, 'gamma': 0.004478892883091871, 'reg_alpha': 0.5812928097336709, 'reg_lambda': 0.11034308108788529}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:16:18,989] Trial 34 finished with value: 0.34704327103515176 and parameters: {'n_estimators': 478, 'max_depth': 4, 'learning_rate': 0.2118629169676653, 'subsample': 0.5009207732420801, 'colsample_bytree': 0.502514075540963, 'gamma': 6.586910847084362e-07, 'reg_alpha': 0.10381056746002591, 'reg_lambda': 0.0012003048154614069}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:16:39,875] Trial 35 finished with value: 0.35001474153681866 and parameters: {'n_estimators': 427, 'max_depth': 5, 'learning_rate': 0.1457162777780062, 'subsample': 0.5746249564289688, 'colsample_bytree': 0.7542712312646053, 'gamma': 0.02634661620288414, 'reg_alpha': 1.6289226148048452, 'reg_lambda': 5.987857669125579e-05}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:17:29,694] Trial 36 finished with value: 0.3445657760276144 and parameters: {'n_estimators': 499, 'max_depth': 9, 'learning_rate': 0.2518647175889195, 'subsample': 0.620393569796222, 'colsample_bytree': 0.5794137958544515, 'gamma': 0.32519964275726826, 'reg_alpha': 9.909872821102178, 'reg_lambda': 0.01064772088263231}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:17:34,895] Trial 37 finished with value: 0.3637445854733743 and parameters: {'n_estimators': 160, 'max_depth': 9, 'learning_rate': 0.2598767099369181, 'subsample': 0.6275770625116008, 'colsample_bytree': 0.5644904523591503, 'gamma': 8.464634073263541, 'reg_alpha': 0.4524057058470053, 'reg_lambda': 0.022904565606480444}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:18:25,316] Trial 38 finished with value: 0.351116683497662 and parameters: {'n_estimators': 386, 'max_depth': 9, 'learning_rate': 0.11508087362203888, 'subsample': 0.6705882375269059, 'colsample_bytree': 0.6897701594029911, 'gamma': 0.769725405491444, 'reg_alpha': 1.7602564955749423e-07, 'reg_lambda': 0.23762285890886428}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:19:30,338] Trial 39 finished with value: 0.35182322282999534 and parameters: {'n_estimators': 498, 'max_depth': 8, 'learning_rate': 0.16433460191487825, 'subsample': 0.7008330769124067, 'colsample_bytree': 0.9879071628396172, 'gamma': 0.2925151485193398, 'reg_alpha': 6.921832114821532e-05, 'reg_lambda': 0.009441324260177757}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:19:53,212] Trial 40 finished with value: 0.3451900060649217 and parameters: {'n_estimators': 232, 'max_depth': 10, 'learning_rate': 0.29969992835793485, 'subsample': 0.7346753144335871, 'colsample_bytree': 0.921016827031552, 'gamma': 1.8306890127490407, 'reg_alpha': 0.04646215747358809, 'reg_lambda': 0.00018724610228529744}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:20:22,605] Trial 41 finished with value: 0.34621154682442545 and parameters: {'n_estimators': 222, 'max_depth': 10, 'learning_rate': 0.28772348890221827, 'subsample': 0.8039279015865329, 'colsample_bytree': 0.9289732976057912, 'gamma': 1.1198990341052288, 'reg_alpha': 2.9718102049749073, 'reg_lambda': 4.892096331630318e-06}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:20:40,363] Trial 42 finished with value: 0.3493913016924308 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.25717487533115685, 'subsample': 0.5259862442880066, 'colsample_bytree': 0.8876825914547711, 'gamma': 3.3702145464837803, 'reg_alpha': 0.05895841635282431, 'reg_lambda': 0.0002509043101774549}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:20:57,822] Trial 43 finished with value: 0.3505662956682871 and parameters: {'n_estimators': 276, 'max_depth': 10, 'learning_rate': 0.19690580818103326, 'subsample': 0.7672478951167849, 'colsample_bytree': 0.5971939380067915, 'gamma': 2.4402119300640464, 'reg_alpha': 0.007299574049556454, 'reg_lambda': 0.0006336093065993013}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:21:32,546] Trial 44 finished with value: 0.3482505524381555 and parameters: {'n_estimators': 226, 'max_depth': 9, 'learning_rate': 0.2972742135665616, 'subsample': 0.8434718425145458, 'colsample_bytree': 0.7931559174654595, 'gamma': 0.19634220888868092, 'reg_alpha': 0.1774565719925283, 'reg_lambda': 0.03841257614283735}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:21:47,638] Trial 45 finished with value: 0.34584190154739514 and parameters: {'n_estimators': 189, 'max_depth': 9, 'learning_rate': 0.2600300812995415, 'subsample': 0.7308664936154047, 'colsample_bytree': 0.9210338644169612, 'gamma': 3.0932543680153644, 'reg_alpha': 1.1417326924204332, 'reg_lambda': 4.415536633605169e-05}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:22:46,404] Trial 46 finished with value: 0.34704309744381456 and parameters: {'n_estimators': 309, 'max_depth': 10, 'learning_rate': 0.24252080106114296, 'subsample': 0.9554137385604626, 'colsample_bytree': 0.6423629723855289, 'gamma': 9.468969252382714e-05, 'reg_alpha': 3.7989067718140674, 'reg_lambda': 0.00013815892283399435}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:24:00,537] Trial 47 finished with value: 0.35080779357114733 and parameters: {'n_estimators': 480, 'max_depth': 9, 'learning_rate': 0.1924787300583891, 'subsample': 0.6525095024077675, 'colsample_bytree': 0.7517554343917607, 'gamma': 0.023803560352565284, 'reg_alpha': 0.3051999493946422, 'reg_lambda': 0.008381619387366385}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:24:30,931] Trial 48 finished with value: 0.3465406275730434 and parameters: {'n_estimators': 258, 'max_depth': 8, 'learning_rate': 0.26085153869048383, 'subsample': 0.5831832942165347, 'colsample_bytree': 0.8323319871319709, 'gamma': 0.09348124473901788, 'reg_alpha': 0.0020821439323072373, 'reg_lambda': 0.0017709566838260652}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:25:44,937] Trial 49 finished with value: 0.35042681594201 and parameters: {'n_estimators': 421, 'max_depth': 10, 'learning_rate': 0.0988211790333868, 'subsample': 0.677879969474697, 'colsample_bytree': 0.5674197745532412, 'gamma': 0.00017617598704402343, 'reg_alpha': 4.461945237543668, 'reg_lambda': 3.471784910499147e-06}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:25:59,371] Trial 50 finished with value: 0.34813704934356193 and parameters: {'n_estimators': 120, 'max_depth': 9, 'learning_rate': 0.20921063638699697, 'subsample': 0.6257359332544722, 'colsample_bytree': 0.615477537547611, 'gamma': 0.013858161071907345, 'reg_alpha': 0.024072657208286434, 'reg_lambda': 0.0006710698441728383}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:26:16,109] Trial 51 finished with value: 0.3487128772470214 and parameters: {'n_estimators': 458, 'max_depth': 4, 'learning_rate': 0.22343890452358342, 'subsample': 0.5276944932915423, 'colsample_bytree': 0.5389551890181683, 'gamma': 0.002255237543791425, 'reg_alpha': 9.893837482648768, 'reg_lambda': 0.003568628836008343}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:26:51,470] Trial 52 finished with value: 0.3484069073073144 and parameters: {'n_estimators': 475, 'max_depth': 7, 'learning_rate': 0.2722533572249483, 'subsample': 0.5614556838442313, 'colsample_bytree': 0.47219296989598125, 'gamma': 0.0004195453561010082, 'reg_alpha': 1.097692261209305, 'reg_lambda': 0.0023655964250434303}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:27:21,236] Trial 53 finished with value: 0.3432287279056213 and parameters: {'n_estimators': 487, 'max_depth': 6, 'learning_rate': 0.1828146834575947, 'subsample': 0.5204429261485295, 'colsample_bytree': 0.5494224941156942, 'gamma': 0.3884174274225489, 'reg_alpha': 5.288272715630251, 'reg_lambda': 0.04179967683619861}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:28:37,606] Trial 54 finished with value: 0.3480392435875259 and parameters: {'n_estimators': 481, 'max_depth': 10, 'learning_rate': 0.15391561111098523, 'subsample': 0.5206665987686224, 'colsample_bytree': 0.6500475045157119, 'gamma': 0.3202076678018559, 'reg_alpha': 4.228484818388963, 'reg_lambda': 0.17308914722033797}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:29:03,635] Trial 55 finished with value: 0.3437448650084388 and parameters: {'n_estimators': 488, 'max_depth': 6, 'learning_rate': 0.18432084232096596, 'subsample': 0.5984882609613867, 'colsample_bytree': 0.6243335603552572, 'gamma': 2.2409616005150164, 'reg_alpha': 0.7194255488192965, 'reg_lambda': 0.043479926853489635}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:29:14,479] Trial 56 finished with value: 0.3537027700708294 and parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.17575020246335699, 'subsample': 0.5981526887988846, 'colsample_bytree': 0.5183104542460951, 'gamma': 2.045289940745565, 'reg_alpha': 0.505147999907385, 'reg_lambda': 0.03285112164920906}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:29:25,024] Trial 57 finished with value: 0.36739217965597315 and parameters: {'n_estimators': 498, 'max_depth': 6, 'learning_rate': 0.1368657158804054, 'subsample': 0.6375632383102938, 'colsample_bytree': 0.5899244375077611, 'gamma': 9.936956338994067, 'reg_alpha': 0.9799017752589583, 'reg_lambda': 1.362640689879042}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:30:01,182] Trial 58 finished with value: 0.3457239827410963 and parameters: {'n_estimators': 444, 'max_depth': 7, 'learning_rate': 0.12561529758827475, 'subsample': 0.5748189907227002, 'colsample_bytree': 0.6815390882132693, 'gamma': 0.4834357072090933, 'reg_alpha': 3.4803335035626993e-06, 'reg_lambda': 4.508005869403996}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:30:24,331] Trial 59 finished with value: 0.34704874464951385 and parameters: {'n_estimators': 489, 'max_depth': 6, 'learning_rate': 0.19026155774899975, 'subsample': 0.7436142344942553, 'colsample_bytree': 0.4054373061903883, 'gamma': 1.3425342593097245, 'reg_alpha': 0.20005789137674554, 'reg_lambda': 0.7389071415062121}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:30:38,904] Trial 60 finished with value: 0.35334359459941367 and parameters: {'n_estimators': 461, 'max_depth': 6, 'learning_rate': 0.15610851168167336, 'subsample': 0.6035861337893741, 'colsample_bytree': 0.6232980715510732, 'gamma': 4.450853517931649, 'reg_alpha': 2.1259188002429306, 'reg_lambda': 0.007730125437791551}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:31:30,491] Trial 61 finished with value: 0.3483860899578166 and parameters: {'n_estimators': 468, 'max_depth': 8, 'learning_rate': 0.2408619548649883, 'subsample': 0.5411985973260293, 'colsample_bytree': 0.6023598911341134, 'gamma': 0.1027336179433263, 'reg_alpha': 4.9157556705507925, 'reg_lambda': 0.04625712637864999}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:31:53,238] Trial 62 finished with value: 0.3447167392802343 and parameters: {'n_estimators': 483, 'max_depth': 5, 'learning_rate': 0.20833495119957637, 'subsample': 0.5149221857834685, 'colsample_bytree': 0.5735751603858594, 'gamma': 5.65704325976698e-05, 'reg_alpha': 5.96172262522485, 'reg_lambda': 0.0766141258687654}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:32:13,245] Trial 63 finished with value: 0.34834850784329663 and parameters: {'n_estimators': 440, 'max_depth': 5, 'learning_rate': 0.20835882595750194, 'subsample': 0.5122936845351733, 'colsample_bytree': 0.5530058858543501, 'gamma': 0.2003022543946678, 'reg_alpha': 1.7843953768469367, 'reg_lambda': 0.10590005892132803}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:32:31,990] Trial 64 finished with value: 0.3670818993775542 and parameters: {'n_estimators': 484, 'max_depth': 5, 'learning_rate': 0.03735878724747209, 'subsample': 0.5370030926137166, 'colsample_bytree': 0.5800032869050287, 'gamma': 1.0425019011651597, 'reg_alpha': 0.33053335227346003, 'reg_lambda': 0.015038874407388308}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:32:47,489] Trial 65 finished with value: 0.3499060191619341 and parameters: {'n_estimators': 451, 'max_depth': 4, 'learning_rate': 0.18391444237237298, 'subsample': 0.5649200989184746, 'colsample_bytree': 0.52081470929105, 'gamma': 5.338564708910534e-06, 'reg_alpha': 0.786162861812011, 'reg_lambda': 0.06214949712262453}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:33:08,541] Trial 66 finished with value: 0.3493240920537848 and parameters: {'n_estimators': 418, 'max_depth': 5, 'learning_rate': 0.16370759881075167, 'subsample': 0.5011286139753459, 'colsample_bytree': 0.7330238442354395, 'gamma': 2.3993257471375504e-05, 'reg_alpha': 5.210790075333743, 'reg_lambda': 0.02455414044669254}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:33:39,197] Trial 67 finished with value: 0.3468912329141146 and parameters: {'n_estimators': 403, 'max_depth': 7, 'learning_rate': 0.226359072936733, 'subsample': 0.5192645560772104, 'colsample_bytree': 0.4824197969687106, 'gamma': 4.264059202231693e-05, 'reg_alpha': 2.2885542195558872, 'reg_lambda': 0.3759227186127133}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:34:05,989] Trial 68 finished with value: 0.35636686864687717 and parameters: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.05246204290398064, 'subsample': 0.5545744834356499, 'colsample_bytree': 0.5640958254647956, 'gamma': 1.277444925683695e-08, 'reg_alpha': 6.0478796116798055, 'reg_lambda': 0.07740297045781838}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:34:17,481] Trial 69 finished with value: 0.35642555862020786 and parameters: {'n_estimators': 244, 'max_depth': 6, 'learning_rate': 0.14333841956571694, 'subsample': 0.5802319443305037, 'colsample_bytree': 0.6308211848104404, 'gamma': 4.4059622632962485, 'reg_alpha': 0.09048963064248439, 'reg_lambda': 0.17671260770151653}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:34:36,619] Trial 70 finished with value: 0.34806577131066574 and parameters: {'n_estimators': 468, 'max_depth': 5, 'learning_rate': 0.2036717499972747, 'subsample': 0.5385449848714001, 'colsample_bytree': 0.4495729059000671, 'gamma': 0.679022195877038, 'reg_alpha': 0.050579746186466705, 'reg_lambda': 0.0075561691361927235}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:35:34,720] Trial 71 finished with value: 0.3483691437253583 and parameters: {'n_estimators': 485, 'max_depth': 8, 'learning_rate': 0.2843345897116773, 'subsample': 0.5103725134205306, 'colsample_bytree': 0.6691981603028873, 'gamma': 5.419844780942136e-05, 'reg_alpha': 7.0330001066367345, 'reg_lambda': 0.015259945773267416}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:36:40,801] Trial 72 finished with value: 0.3516090796529625 and parameters: {'n_estimators': 455, 'max_depth': 9, 'learning_rate': 0.2742478409804828, 'subsample': 0.5318476826030629, 'colsample_bytree': 0.6100652889575894, 'gamma': 0.0003553987331429264, 'reg_alpha': 2.797530534037861, 'reg_lambda': 0.0012675982523129372}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:38:01,367] Trial 73 finished with value: 0.34811327422712357 and parameters: {'n_estimators': 433, 'max_depth': 10, 'learning_rate': 0.23689965495419915, 'subsample': 0.5156208155179234, 'colsample_bytree': 0.6402019070333533, 'gamma': 1.1934486357571105e-05, 'reg_alpha': 1.5269532726517325, 'reg_lambda': 0.00457968525244817}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:38:45,733] Trial 74 finished with value: 0.3456356728048652 and parameters: {'n_estimators': 488, 'max_depth': 7, 'learning_rate': 0.17118658131955838, 'subsample': 0.5883787151995217, 'colsample_bytree': 0.6999755930898598, 'gamma': 0.0006567843582813805, 'reg_alpha': 9.900264780558745, 'reg_lambda': 0.03946273760845475}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:39:06,962] Trial 75 finished with value: 0.3450399210833941 and parameters: {'n_estimators': 472, 'max_depth': 5, 'learning_rate': 0.21856529952543685, 'subsample': 0.5521534661047176, 'colsample_bytree': 0.5844844118676303, 'gamma': 6.989632524823448e-05, 'reg_alpha': 0.7053115992953959, 'reg_lambda': 0.6658324887149796}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:39:22,937] Trial 76 finished with value: 0.34837094987712613 and parameters: {'n_estimators': 469, 'max_depth': 4, 'learning_rate': 0.18215742589133085, 'subsample': 0.6132267684307736, 'colsample_bytree': 0.5740909313213662, 'gamma': 0.00019142475893678724, 'reg_alpha': 0.49254162936183743, 'reg_lambda': 0.5357635242015631}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:39:38,220] Trial 77 finished with value: 0.3474048176516765 and parameters: {'n_estimators': 343, 'max_depth': 5, 'learning_rate': 0.22286900095092135, 'subsample': 0.5637870122836544, 'colsample_bytree': 0.5372897995299375, 'gamma': 0.05593018274644305, 'reg_alpha': 0.8581467978742644, 'reg_lambda': 2.420852124331299}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:39:59,973] Trial 78 finished with value: 0.34308974976232154 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.18438715817374218, 'subsample': 0.5442905828505827, 'colsample_bytree': 0.5495297517735737, 'gamma': 1.8675303901948084, 'reg_alpha': 0.16738780435088235, 'reg_lambda': 0.15523161020257895}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:40:20,747] Trial 79 finished with value: 0.34566041506927503 and parameters: {'n_estimators': 485, 'max_depth': 5, 'learning_rate': 0.20075875227584702, 'subsample': 0.5456023475949621, 'colsample_bytree': 0.5120214687508235, 'gamma': 0.409537351737849, 'reg_alpha': 0.1340738861784061, 'reg_lambda': 0.15902692396178328}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:40:35,681] Trial 80 finished with value: 0.35060133182115005 and parameters: {'n_estimators': 432, 'max_depth': 4, 'learning_rate': 0.16346509742891221, 'subsample': 0.5681883427688278, 'colsample_bytree': 0.5550522350648794, 'gamma': 1.8342780628395486e-07, 'reg_alpha': 0.321693764957242, 'reg_lambda': 9.542710232379424}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:40:43,052] Trial 81 finished with value: 0.36514367653024127 and parameters: {'n_estimators': 282, 'max_depth': 5, 'learning_rate': 0.18954833067148333, 'subsample': 0.878078563363816, 'colsample_bytree': 0.6030074784976306, 'gamma': 5.516516624391164, 'reg_alpha': 0.011864309251728603, 'reg_lambda': 0.281425344762789}. Best is trial 31 with value: 0.34266024463931.\n[I 2025-11-28 06:41:04,735] Trial 82 finished with value: 0.3424833025706795 and parameters: {'n_estimators': 491, 'max_depth': 5, 'learning_rate': 0.21686268622292826, 'subsample': 0.5525800241980569, 'colsample_bytree': 0.5864315116436489, 'gamma': 1.459068523957759, 'reg_alpha': 0.0005270933696306975, 'reg_lambda': 0.07852312052461549}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:41:26,827] Trial 83 finished with value: 0.3475329061681055 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.21479682185745938, 'subsample': 0.5538076197101176, 'colsample_bytree': 0.5876149172294869, 'gamma': 0.20038083310066185, 'reg_alpha': 0.00020272837241006852, 'reg_lambda': 0.05506084148378206}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:41:52,953] Trial 84 finished with value: 0.3454854438210112 and parameters: {'n_estimators': 473, 'max_depth': 6, 'learning_rate': 0.17254094544468476, 'subsample': 0.5899337483897616, 'colsample_bytree': 0.5566128173180761, 'gamma': 1.7913266868505093, 'reg_alpha': 0.0005940021245786155, 'reg_lambda': 0.7767291156039827}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:42:14,048] Trial 85 finished with value: 0.34782936538268205 and parameters: {'n_estimators': 489, 'max_depth': 5, 'learning_rate': 0.19742809351421278, 'subsample': 0.5005016318220976, 'colsample_bytree': 0.5453711130761083, 'gamma': 0.8060683918038438, 'reg_alpha': 2.620875584873637e-05, 'reg_lambda': 0.09815059416787479}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:42:29,650] Trial 86 finished with value: 0.3491153282824231 and parameters: {'n_estimators': 452, 'max_depth': 4, 'learning_rate': 0.21566713740707127, 'subsample': 0.533895907795553, 'colsample_bytree': 0.5755861078769814, 'gamma': 6.349080722999105e-05, 'reg_alpha': 3.0971928283255114e-06, 'reg_lambda': 0.02642137936743448}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:42:51,885] Trial 87 finished with value: 0.3473136748864999 and parameters: {'n_estimators': 476, 'max_depth': 5, 'learning_rate': 0.18173978893218248, 'subsample': 0.5194185207057898, 'colsample_bytree': 0.6234672701527924, 'gamma': 1.6773194006966278e-05, 'reg_alpha': 3.1049615567692537, 'reg_lambda': 0.16165492845742366}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:42:55,681] Trial 88 finished with value: 0.36637060714977876 and parameters: {'n_estimators': 59, 'max_depth': 6, 'learning_rate': 0.23664186508092075, 'subsample': 0.5758206678997232, 'colsample_bytree': 0.5285097820136877, 'gamma': 0.3154722153489687, 'reg_alpha': 0.6054637704177533, 'reg_lambda': 0.011837413808470408}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:43:19,922] Trial 89 finished with value: 0.34851437316754263 and parameters: {'n_estimators': 462, 'max_depth': 6, 'learning_rate': 0.15545211620064453, 'subsample': 0.5468721759664723, 'colsample_bytree': 0.4993908710398519, 'gamma': 0.0027962029666549616, 'reg_alpha': 1.170135793620316e-06, 'reg_lambda': 0.3771587158368714}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:43:39,638] Trial 90 finished with value: 0.37294480906407945 and parameters: {'n_estimators': 491, 'max_depth': 5, 'learning_rate': 0.030005071596438695, 'subsample': 0.5530509543695383, 'colsample_bytree': 0.6762142864496192, 'gamma': 9.893354278525128e-05, 'reg_alpha': 0.004247687196090252, 'reg_lambda': 1.3299912545555443}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:44:05,478] Trial 91 finished with value: 0.3460359825525364 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.2695030502780691, 'subsample': 0.5286934364243611, 'colsample_bytree': 0.950496950161485, 'gamma': 1.8891336668969998, 'reg_alpha': 0.03460515421907053, 'reg_lambda': 0.06769906313143133}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:44:21,941] Trial 92 finished with value: 0.3509189296538249 and parameters: {'n_estimators': 479, 'max_depth': 5, 'learning_rate': 0.22992868329399807, 'subsample': 0.9176003495358979, 'colsample_bytree': 0.6537070773129527, 'gamma': 1.3576398842936825, 'reg_alpha': 0.0015036520798642921, 'reg_lambda': 0.00017278524042487392}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:44:33,455] Trial 93 finished with value: 0.3545019451887399 and parameters: {'n_estimators': 466, 'max_depth': 4, 'learning_rate': 0.24880541188642052, 'subsample': 0.6325726899956438, 'colsample_bytree': 0.8606505208980646, 'gamma': 5.861963377593481, 'reg_alpha': 0.20411365040184734, 'reg_lambda': 0.019901277375436174}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:44:59,762] Trial 94 finished with value: 0.3470340934628324 and parameters: {'n_estimators': 446, 'max_depth': 6, 'learning_rate': 0.2034291281075511, 'subsample': 0.5184594479269573, 'colsample_bytree': 0.5830763765487675, 'gamma': 0.15524080253050368, 'reg_alpha': 1.2591914473466377, 'reg_lambda': 0.03647196141542844}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:45:13,326] Trial 95 finished with value: 0.3562692894021542 and parameters: {'n_estimators': 490, 'max_depth': 3, 'learning_rate': 0.18888613612347763, 'subsample': 0.6070917526809858, 'colsample_bytree': 0.5992573981757137, 'gamma': 2.810699586342515, 'reg_alpha': 0.06633299356934258, 'reg_lambda': 0.2668589011050523}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:45:23,055] Trial 96 finished with value: 0.35262821781300835 and parameters: {'n_estimators': 211, 'max_depth': 5, 'learning_rate': 0.21503696915469284, 'subsample': 0.6561892137986189, 'colsample_bytree': 0.6327388204470559, 'gamma': 0.535786140088645, 'reg_alpha': 6.919731111155173e-05, 'reg_lambda': 0.006233066914647184}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:45:56,837] Trial 97 finished with value: 0.34483554125908744 and parameters: {'n_estimators': 472, 'max_depth': 6, 'learning_rate': 0.17860898554231938, 'subsample': 0.5382815772755334, 'colsample_bytree': 0.796682404468691, 'gamma': 0.07552857050224779, 'reg_alpha': 4.8840793963464, 'reg_lambda': 2.9916265859348702e-05}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:46:26,822] Trial 98 finished with value: 0.3469847557281396 and parameters: {'n_estimators': 475, 'max_depth': 6, 'learning_rate': 0.16805413022667562, 'subsample': 0.5407853713113986, 'colsample_bytree': 0.7429667809785483, 'gamma': 0.0007067721675376635, 'reg_alpha': 5.6112915918718044e-08, 'reg_lambda': 2.0898315162809203e-05}. Best is trial 82 with value: 0.3424833025706795.\n[I 2025-11-28 06:46:58,321] Trial 99 finished with value: 0.34623268555680764 and parameters: {'n_estimators': 457, 'max_depth': 6, 'learning_rate': 0.14249060259932397, 'subsample': 0.5705575077825424, 'colsample_bytree': 0.7745018808204401, 'gamma': 0.07433462662329264, 'reg_alpha': 6.3035774575890535, 'reg_lambda': 3.847026339919211e-07}. Best is trial 82 with value: 0.3424833025706795.\n","output_type":"stream"},{"name":"stdout","text":"Best XGB params: {'n_estimators': 491, 'max_depth': 5, 'learning_rate': 0.21686268622292826, 'subsample': 0.5525800241980569, 'colsample_bytree': 0.5864315116436489, 'gamma': 1.459068523957759, 'reg_alpha': 0.0005270933696306975, 'reg_lambda': 0.07852312052461549} best f2: 0.6575166974293205\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def objective_lgb(trial):\n    params = {\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 6, 128),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.03, 0.3, log = True),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log = True),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log = True),\n\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"random_state\": 42,\n        \"n_jobs\": -1\n    }\n\n    model = LGBMClassifier(**params)\n    scores = cross_val_score(model, X_selected, y, cv = cv, scoring = fbeta_scorer, n_jobs = -1)\n    return 1.0 - scores.mean()\n\nstudy_lgb = optuna.create_study(direction = \"minimize\", sampler = optuna.samplers.TPESampler(seed = 42))\nstudy_lgb.optimize(objective_lgb, n_trials = 100)\nprint(\"Best LGB params:\", study_lgb.best_params, \"best f2:\", 1 - study_lgb.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T06:46:58.327834Z","iopub.execute_input":"2025-11-28T06:46:58.328115Z","iopub.status.idle":"2025-11-28T07:09:34.750503Z","shell.execute_reply.started":"2025-11-28T06:46:58.328095Z","shell.execute_reply":"2025-11-28T07:09:34.749641Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"[I 2025-11-28 06:46:58,333] A new study created in memory with name: no-name-12991c8d-d0dd-4b49-964a-63762a621479\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036127 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n[I 2025-11-28 06:47:14,981] Trial 0 finished with value: 0.35112371447410085 and parameters: {'num_leaves': 52, 'min_child_samples': 96, 'learning_rate': 0.1618509290001068, 'n_estimators': 319, 'subsample': 0.5780093202212182, 'colsample_bytree': 0.49359671220172163, 'reg_alpha': 3.3323645788192616e-08, 'reg_lambda': 0.6245760287469893}. Best is trial 0 with value: 0.35112371447410085.\n[I 2025-11-28 06:47:44,873] Trial 1 finished with value: 0.3574138917767693 and parameters: {'num_leaves': 79, 'min_child_samples': 72, 'learning_rate': 0.031456163175583855, 'n_estimators': 487, 'subsample': 0.9162213204002109, 'colsample_bytree': 0.5274034664069657, 'reg_alpha': 4.329370014459266e-07, 'reg_lambda': 4.4734294104626844e-07}. Best is trial 0 with value: 0.35112371447410085.\n[I 2025-11-28 06:47:53,632] Trial 2 finished with value: 0.36138709910813804 and parameters: {'num_leaves': 43, 'min_child_samples': 55, 'learning_rate': 0.08110848199986004, 'n_estimators': 181, 'subsample': 0.8059264473611898, 'colsample_bytree': 0.4836963163912251, 'reg_alpha': 4.258943089524393e-06, 'reg_lambda': 1.9826980964985924e-05}. Best is trial 0 with value: 0.35112371447410085.\n[I 2025-11-28 06:48:07,781] Trial 3 finished with value: 0.3604208659079481 and parameters: {'num_leaves': 62, 'min_child_samples': 80, 'learning_rate': 0.04751109467735623, 'n_estimators': 281, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.42787024763199866, 'reg_alpha': 0.0029369981104377003, 'reg_lambda': 3.425445902633376e-07}. Best is trial 0 with value: 0.35112371447410085.\n[I 2025-11-28 06:48:17,411] Trial 4 finished with value: 0.3501975889972029 and parameters: {'num_leaves': 14, 'min_child_samples': 96, 'learning_rate': 0.27717450958881745, 'n_estimators': 414, 'subsample': 0.6523068845866853, 'colsample_bytree': 0.45860326840383037, 'reg_alpha': 0.014391207615728067, 'reg_lambda': 9.148975058772307e-05}. Best is trial 4 with value: 0.3501975889972029.\n[I 2025-11-28 06:48:36,748] Trial 5 finished with value: 0.3695784970302701 and parameters: {'num_leaves': 21, 'min_child_samples': 52, 'learning_rate': 0.03247205514450286, 'n_estimators': 460, 'subsample': 0.6293899908000085, 'colsample_bytree': 0.7975133706123891, 'reg_alpha': 6.388511557344611e-06, 'reg_lambda': 0.0004793052550782129}. Best is trial 4 with value: 0.3501975889972029.\n[I 2025-11-28 06:49:06,252] Trial 6 finished with value: 0.3496888342268989 and parameters: {'num_leaves': 73, 'min_child_samples': 22, 'learning_rate': 0.27970864055344435, 'n_estimators': 399, 'subsample': 0.9697494707820946, 'colsample_bytree': 0.9368964102565893, 'reg_alpha': 0.002404915432737351, 'reg_lambda': 1.9809253750493907}. Best is trial 6 with value: 0.3496888342268989.\n[I 2025-11-28 06:49:14,689] Trial 7 finished with value: 0.3953859164910173 and parameters: {'num_leaves': 16, 'min_child_samples': 23, 'learning_rate': 0.033292663683309284, 'n_estimators': 196, 'subsample': 0.6943386448447411, 'colsample_bytree': 0.5628094190643376, 'reg_alpha': 0.28749982347407854, 'reg_lambda': 1.6247252885719427e-05}. Best is trial 6 with value: 0.3496888342268989.\n[I 2025-11-28 06:49:38,774] Trial 8 finished with value: 0.3609342110048728 and parameters: {'num_leaves': 40, 'min_child_samples': 57, 'learning_rate': 0.041499749925659864, 'n_estimators': 411, 'subsample': 0.5372753218398854, 'colsample_bytree': 0.9921321619603104, 'reg_alpha': 0.08916674715636537, 'reg_lambda': 6.143857495033091e-07}. Best is trial 6 with value: 0.3496888342268989.\n[I 2025-11-28 06:49:46,629] Trial 9 finished with value: 0.36407902787109614 and parameters: {'num_leaves': 6, 'min_child_samples': 83, 'learning_rate': 0.15274907837455656, 'n_estimators': 378, 'subsample': 0.8856351733429728, 'colsample_bytree': 0.44442679104045424, 'reg_alpha': 1.683416412018213e-05, 'reg_lambda': 1.1036250149900698e-07}. Best is trial 6 with value: 0.3496888342268989.\n[I 2025-11-28 06:49:55,087] Trial 10 finished with value: 0.345190778052448 and parameters: {'num_leaves': 113, 'min_child_samples': 5, 'learning_rate': 0.27967803877874403, 'n_estimators': 58, 'subsample': 0.9919371080730973, 'colsample_bytree': 0.9445988771695106, 'reg_alpha': 4.3444691085504115, 'reg_lambda': 6.178893500921431}. Best is trial 10 with value: 0.345190778052448.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006877 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005845 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028729 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002882 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031468 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030695 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002585 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034690 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035792 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018248 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031208 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030971 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037335 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018236 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005988 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002827 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031486 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005387 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033288 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002411 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036962 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019746 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039208 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034456 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003459 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036866 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033133 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037407 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040550 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:50:03,890] Trial 11 finished with value: 0.34524619010245416 and parameters: {'num_leaves': 119, 'min_child_samples': 5, 'learning_rate': 0.2947986009059022, 'n_estimators': 57, 'subsample': 0.9869601043141659, 'colsample_bytree': 0.9950907663814265, 'reg_alpha': 7.556243125116112, 'reg_lambda': 5.2052107279562385}. Best is trial 10 with value: 0.345190778052448.\n[I 2025-11-28 06:50:11,389] Trial 12 finished with value: 0.3464557770411777 and parameters: {'num_leaves': 123, 'min_child_samples': 5, 'learning_rate': 0.19933580085944358, 'n_estimators': 51, 'subsample': 0.9917980782940604, 'colsample_bytree': 0.8577618901311783, 'reg_alpha': 9.41874879245652, 'reg_lambda': 0.05547786031884726}. Best is trial 10 with value: 0.345190778052448.\n[I 2025-11-28 06:50:18,378] Trial 13 finished with value: 0.359078443010729 and parameters: {'num_leaves': 127, 'min_child_samples': 8, 'learning_rate': 0.09865579539940637, 'n_estimators': 54, 'subsample': 0.8731522909801657, 'colsample_bytree': 0.6911368287326002, 'reg_alpha': 9.868745464853815, 'reg_lambda': 0.026268548851648064}. Best is trial 10 with value: 0.345190778052448.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006046 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004684 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032607 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004901 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036475 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036815 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033387 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011258 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005764 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030347 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002665 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036521 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036201 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035898 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038165 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036027 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:50:31,205] Trial 14 finished with value: 0.34327195601387583 and parameters: {'num_leaves': 101, 'min_child_samples': 29, 'learning_rate': 0.20946012406339398, 'n_estimators': 121, 'subsample': 0.996021339734391, 'colsample_bytree': 0.8654806210166778, 'reg_alpha': 0.6255688317185165, 'reg_lambda': 9.207736961357973}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:50:43,517] Trial 15 finished with value: 0.3504582724558599 and parameters: {'num_leaves': 99, 'min_child_samples': 32, 'learning_rate': 0.20034833018713577, 'n_estimators': 144, 'subsample': 0.9221534889950529, 'colsample_bytree': 0.8075588525957713, 'reg_alpha': 0.39909064972762814, 'reg_lambda': 0.06275624421868707}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:50:54,440] Trial 16 finished with value: 0.3538307824315797 and parameters: {'num_leaves': 96, 'min_child_samples': 38, 'learning_rate': 0.10762270410530717, 'n_estimators': 124, 'subsample': 0.8352725713024433, 'colsample_bytree': 0.6924147343320586, 'reg_alpha': 0.6490255464867823, 'reg_lambda': 0.0056913618241371715}. Best is trial 14 with value: 0.34327195601387583.\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030941 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031774 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032258 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031905 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036745 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037792 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006566 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039940 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031601 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037918 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041296 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038456 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037888 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036001 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.049984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038959 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:51:07,594] Trial 17 finished with value: 0.34563776558859804 and parameters: {'num_leaves': 101, 'min_child_samples': 19, 'learning_rate': 0.20780617399132628, 'n_estimators': 125, 'subsample': 0.7220668180434928, 'colsample_bytree': 0.8887709727296994, 'reg_alpha': 0.0004590883367895594, 'reg_lambda': 8.131078351312306}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:51:26,436] Trial 18 finished with value: 0.35245526718419584 and parameters: {'num_leaves': 108, 'min_child_samples': 37, 'learning_rate': 0.13839385071978752, 'n_estimators': 226, 'subsample': 0.9478668969549888, 'colsample_bytree': 0.7764518959689806, 'reg_alpha': 0.039553805820462605, 'reg_lambda': 0.3683649250652756}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:51:36,300] Trial 19 finished with value: 0.36037742404288475 and parameters: {'num_leaves': 81, 'min_child_samples': 15, 'learning_rate': 0.07146892101381584, 'n_estimators': 114, 'subsample': 0.7678232809190041, 'colsample_bytree': 0.6094623803975799, 'reg_alpha': 0.7842408650615766, 'reg_lambda': 0.00231120659889525}. Best is trial 14 with value: 0.34327195601387583.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039443 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018917 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034674 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018490 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035780 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017694 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037869 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040060 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017949 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034871 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035542 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017791 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037307 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017845 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031472 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036745 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006566 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:51:55,393] Trial 20 finished with value: 0.34874317488589734 and parameters: {'num_leaves': 86, 'min_child_samples': 31, 'learning_rate': 0.22913914045667033, 'n_estimators': 245, 'subsample': 0.861815572925765, 'colsample_bytree': 0.917222815104249, 'reg_alpha': 0.00010396761370918986, 'reg_lambda': 0.5777437508670802}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:52:06,738] Trial 21 finished with value: 0.34552752439337886 and parameters: {'num_leaves': 114, 'min_child_samples': 11, 'learning_rate': 0.27032163429676376, 'n_estimators': 86, 'subsample': 0.9981126922026228, 'colsample_bytree': 0.9748468403715274, 'reg_alpha': 3.3023487211818527, 'reg_lambda': 6.538933290731444}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:52:16,160] Trial 22 finished with value: 0.34628597895723234 and parameters: {'num_leaves': 116, 'min_child_samples': 43, 'learning_rate': 0.29577675488488736, 'n_estimators': 86, 'subsample': 0.9369197726010635, 'colsample_bytree': 0.8569043423593898, 'reg_alpha': 2.3450029639817456, 'reg_lambda': 1.1026131857133278e-08}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:52:32,856] Trial 23 finished with value: 0.3455416188256295 and parameters: {'num_leaves': 94, 'min_child_samples': 25, 'learning_rate': 0.23524283125233772, 'n_estimators': 163, 'subsample': 0.9932339448598012, 'colsample_bytree': 0.942715843739254, 'reg_alpha': 0.10095757047050852, 'reg_lambda': 8.038061038966326}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:52:43,070] Trial 24 finished with value: 0.3537846356622537 and parameters: {'num_leaves': 108, 'min_child_samples': 14, 'learning_rate': 0.17583086184866464, 'n_estimators': 84, 'subsample': 0.9067087287794952, 'colsample_bytree': 0.9916749065272998, 'reg_alpha': 0.00997873841078996, 'reg_lambda': 0.2097353710463464}. Best is trial 14 with value: 0.34327195601387583.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038016 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035543 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037803 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020008 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043095 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036204 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038308 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036136 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038530 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037658 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037692 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011066 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011322 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019598 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038195 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:52:51,537] Trial 25 finished with value: 0.3544794041504037 and parameters: {'num_leaves': 128, 'min_child_samples': 5, 'learning_rate': 0.12697597510059924, 'n_estimators': 52, 'subsample': 0.9572142311251033, 'colsample_bytree': 0.8631430330500817, 'reg_alpha': 1.7133251672341698, 'reg_lambda': 2.487632698794177}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:53:01,671] Trial 26 finished with value: 0.34466041227957633 and parameters: {'num_leaves': 116, 'min_child_samples': 46, 'learning_rate': 0.23883515802751532, 'n_estimators': 94, 'subsample': 0.84375473407869, 'colsample_bytree': 0.7523616508623473, 'reg_alpha': 7.31516956222854, 'reg_lambda': 1.4747130072368995}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:53:15,644] Trial 27 finished with value: 0.3478591138995808 and parameters: {'num_leaves': 90, 'min_child_samples': 47, 'learning_rate': 0.23136152136237817, 'n_estimators': 205, 'subsample': 0.8364173367413742, 'colsample_bytree': 0.7206694294672162, 'reg_alpha': 0.11346505660106715, 'reg_lambda': 0.01615365931705821}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:53:29,294] Trial 28 finished with value: 0.3469109094475453 and parameters: {'num_leaves': 109, 'min_child_samples': 63, 'learning_rate': 0.1853096417702606, 'n_estimators': 158, 'subsample': 0.7360559376559597, 'colsample_bytree': 0.7502445944642884, 'reg_alpha': 1.3295811155964392, 'reg_lambda': 1.2576058272567503}. Best is trial 14 with value: 0.34327195601387583.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013233 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037282 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018655 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036679 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037086 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034992 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018402 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035797 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036830 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035635 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037853 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036968 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038509 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035480 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036216 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036202 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033952 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:53:36,316] Trial 29 finished with value: 0.3560655280249013 and parameters: {'num_leaves': 66, 'min_child_samples': 30, 'learning_rate': 0.15837187726627797, 'n_estimators': 103, 'subsample': 0.8911472904119923, 'colsample_bytree': 0.6232931827415774, 'reg_alpha': 3.0439381964747037e-07, 'reg_lambda': 0.2125304593888528}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:54:04,795] Trial 30 finished with value: 0.35391018775072225 and parameters: {'num_leaves': 106, 'min_child_samples': 63, 'learning_rate': 0.06869612210277144, 'n_estimators': 330, 'subsample': 0.8427026952283718, 'colsample_bytree': 0.8372646594098372, 'reg_alpha': 2.4249713988677598e-08, 'reg_lambda': 0.7151276229467174}. Best is trial 14 with value: 0.34327195601387583.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036746 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037023 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039655 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012218 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039055 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037760 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036863 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037570 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019077 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037004 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035998 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038222 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019304 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037926 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035711 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019256 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035984 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018511 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035915 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034564 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036447 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034642 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018007 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034016 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034656 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018324 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033792 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017170 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035466 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036339 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018595 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:54:14,366] Trial 31 finished with value: 0.34448506273360435 and parameters: {'num_leaves': 119, 'min_child_samples': 16, 'learning_rate': 0.2487875906088435, 'n_estimators': 72, 'subsample': 0.9603903212252876, 'colsample_bytree': 0.9012364216581861, 'reg_alpha': 8.931247902933064, 'reg_lambda': 2.0975428420274427}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:54:25,145] Trial 32 finished with value: 0.3475774899094549 and parameters: {'num_leaves': 119, 'min_child_samples': 17, 'learning_rate': 0.2406826770069502, 'n_estimators': 91, 'subsample': 0.946712758405013, 'colsample_bytree': 0.8930116280368003, 'reg_alpha': 3.7912818021895283, 'reg_lambda': 0.13322787050096757}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:54:40,348] Trial 33 finished with value: 0.3462810769239957 and parameters: {'num_leaves': 113, 'min_child_samples': 29, 'learning_rate': 0.24603903573276795, 'n_estimators': 144, 'subsample': 0.9207891453175421, 'colsample_bytree': 0.9129554532366954, 'reg_alpha': 0.3894047190458378, 'reg_lambda': 1.5259616723871021}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:54:56,523] Trial 34 finished with value: 0.3491600125455758 and parameters: {'num_leaves': 102, 'min_child_samples': 39, 'learning_rate': 0.11950490866456412, 'n_estimators': 173, 'subsample': 0.7977503290017716, 'colsample_bytree': 0.8302066166451872, 'reg_alpha': 1.4800729142129876, 'reg_lambda': 2.1155025982899156}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:55:11,368] Trial 35 finished with value: 0.3482406079465432 and parameters: {'num_leaves': 55, 'min_child_samples': 48, 'learning_rate': 0.1788805436392825, 'n_estimators': 284, 'subsample': 0.9642843812873553, 'colsample_bytree': 0.7658013366600113, 'reg_alpha': 0.02101840495593215, 'reg_lambda': 0.6723638659120866}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:55:20,545] Trial 36 finished with value: 0.3507974127114456 and parameters: {'num_leaves': 121, 'min_child_samples': 25, 'learning_rate': 0.21401180599138148, 'n_estimators': 76, 'subsample': 0.9090616856111845, 'colsample_bytree': 0.9631808332413613, 'reg_alpha': 0.19455270740782685, 'reg_lambda': 0.0030563772969054884}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:55:30,660] Trial 37 finished with value: 0.34548290220096123 and parameters: {'num_leaves': 78, 'min_child_samples': 13, 'learning_rate': 0.25584136613262737, 'n_estimators': 118, 'subsample': 0.9733235933982991, 'colsample_bytree': 0.8113399397137858, 'reg_alpha': 3.9136255595545144, 'reg_lambda': 3.4094294761883877}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:55:38,866] Trial 38 finished with value: 0.35671440793680975 and parameters: {'num_leaves': 89, 'min_child_samples': 20, 'learning_rate': 0.14664289891115173, 'n_estimators': 142, 'subsample': 0.6337923278103027, 'colsample_bytree': 0.40160077676636885, 'reg_alpha': 0.00519569534728207, 'reg_lambda': 0.0006288491690734703}. Best is trial 14 with value: 0.34327195601387583.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019450 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036171 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035284 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036238 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037455 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038403 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037117 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041691 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040717 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036849 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018669 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036930 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036599 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031014 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037601 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:55:47,130] Trial 39 finished with value: 0.3539832941556944 and parameters: {'num_leaves': 35, 'min_child_samples': 35, 'learning_rate': 0.1700859485292446, 'n_estimators': 186, 'subsample': 0.9337227437587067, 'colsample_bytree': 0.6519804513447657, 'reg_alpha': 0.055887544602928, 'reg_lambda': 3.796229852684374e-05}. Best is trial 14 with value: 0.34327195601387583.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034711 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018634 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034887 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018481 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033889 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018416 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035120 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012096 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035405 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017422 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.028244 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015808 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033901 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035564 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017910 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:56:22,743] Trial 40 finished with value: 0.3492263868597716 and parameters: {'num_leaves': 111, 'min_child_samples': 44, 'learning_rate': 0.29969022951164365, 'n_estimators': 485, 'subsample': 0.7713442114682522, 'colsample_bytree': 0.7307486019605397, 'reg_alpha': 0.0008311649356041842, 'reg_lambda': 5.531251073932811e-06}. Best is trial 14 with value: 0.34327195601387583.\n[I 2025-11-28 06:56:32,309] Trial 41 finished with value: 0.34288142037420966 and parameters: {'num_leaves': 122, 'min_child_samples': 9, 'learning_rate': 0.26902133991222255, 'n_estimators': 65, 'subsample': 0.9755748989517207, 'colsample_bytree': 0.9552524858146166, 'reg_alpha': 9.119796653187974, 'reg_lambda': 8.80974244480979}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:56:42,617] Trial 42 finished with value: 0.3433404068871073 and parameters: {'num_leaves': 125, 'min_child_samples': 10, 'learning_rate': 0.2672223293715309, 'n_estimators': 73, 'subsample': 0.9680120577543118, 'colsample_bytree': 0.9459004784936361, 'reg_alpha': 5.036590097226218, 'reg_lambda': 3.0479715570796952}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:56:55,987] Trial 43 finished with value: 0.34511308748833014 and parameters: {'num_leaves': 125, 'min_child_samples': 9, 'learning_rate': 0.25922681828187744, 'n_estimators': 102, 'subsample': 0.964674091743739, 'colsample_bytree': 0.8930868978252363, 'reg_alpha': 0.7274668527822411, 'reg_lambda': 9.856352422331087}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:57:05,426] Trial 44 finished with value: 0.34515767782600704 and parameters: {'num_leaves': 119, 'min_child_samples': 25, 'learning_rate': 0.21492219359965656, 'n_estimators': 72, 'subsample': 0.5007250058963767, 'colsample_bytree': 0.9210433870492871, 'reg_alpha': 8.919246635773344, 'reg_lambda': 0.10299505205093594}. Best is trial 41 with value: 0.34288142037420966.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036310 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038260 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038502 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011118 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036053 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035274 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037571 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037582 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031224 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035906 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036381 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035877 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038140 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036286 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036918 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:57:18,295] Trial 45 finished with value: 0.34833872482613937 and parameters: {'num_leaves': 128, 'min_child_samples': 18, 'learning_rate': 0.2652494119391497, 'n_estimators': 103, 'subsample': 0.8944059436081964, 'colsample_bytree': 0.9660277089913349, 'reg_alpha': 1.7073980967731708, 'reg_lambda': 0.7652132968519625}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:57:26,063] Trial 46 finished with value: 0.35070038413407856 and parameters: {'num_leaves': 104, 'min_child_samples': 75, 'learning_rate': 0.19428067891074738, 'n_estimators': 70, 'subsample': 0.8696618530626404, 'colsample_bytree': 0.8792441111312298, 'reg_alpha': 0.2690830591637435, 'reg_lambda': 3.079283749904838}. Best is trial 41 with value: 0.34288142037420966.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037768 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011150 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038949 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036208 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037737 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036515 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032109 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036020 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036858 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037247 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037931 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037505 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036987 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037176 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007191 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037218 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032745 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:57:37,779] Trial 47 finished with value: 0.36120940145561686 and parameters: {'num_leaves': 122, 'min_child_samples': 55, 'learning_rate': 0.05091201071377662, 'n_estimators': 131, 'subsample': 0.6771291535654584, 'colsample_bytree': 0.5306123057968023, 'reg_alpha': 9.350677221107256, 'reg_lambda': 0.022617784285899473}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:57:49,265] Trial 48 finished with value: 0.34381811557492203 and parameters: {'num_leaves': 116, 'min_child_samples': 88, 'learning_rate': 0.22642425728409288, 'n_estimators': 104, 'subsample': 0.5964192686896229, 'colsample_bytree': 0.9344345935749105, 'reg_alpha': 3.6931141395698073, 'reg_lambda': 0.3412828757911305}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:58:22,817] Trial 49 finished with value: 0.34872458724443045 and parameters: {'num_leaves': 96, 'min_child_samples': 10, 'learning_rate': 0.21117915832159834, 'n_estimators': 354, 'subsample': 0.5742483605583295, 'colsample_bytree': 0.9441982218944477, 'reg_alpha': 0.7624135775745189, 'reg_lambda': 0.31482364744149993}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:58:43,306] Trial 50 finished with value: 0.34696847327698577 and parameters: {'num_leaves': 123, 'min_child_samples': 99, 'learning_rate': 0.27749855920192795, 'n_estimators': 212, 'subsample': 0.6071685561693618, 'colsample_bytree': 0.9160922932287517, 'reg_alpha': 0.20355827879559765, 'reg_lambda': 0.04897884987050577}. Best is trial 41 with value: 0.34288142037420966.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033780 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018178 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035185 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018375 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033861 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018013 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034344 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035429 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035434 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017763 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005750 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037569 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017811 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034911 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019228 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033253 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035107 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:58:51,791] Trial 51 finished with value: 0.34463208543471613 and parameters: {'num_leaves': 116, 'min_child_samples': 87, 'learning_rate': 0.2361989681810032, 'n_estimators': 68, 'subsample': 0.9706707625680668, 'colsample_bytree': 0.9659934299342441, 'reg_alpha': 3.8343401346622716, 'reg_lambda': 1.0689247503530876}. Best is trial 41 with value: 0.34288142037420966.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019762 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035705 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036868 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039145 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036714 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037776 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019160 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038602 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038637 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032486 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003159 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036373 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038238 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038944 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036772 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020188 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038584 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 06:59:00,360] Trial 52 finished with value: 0.34557941296614136 and parameters: {'num_leaves': 117, 'min_child_samples': 91, 'learning_rate': 0.2209594449857796, 'n_estimators': 66, 'subsample': 0.9779454354401145, 'colsample_bytree': 0.9665465621784151, 'reg_alpha': 3.5076166457715403, 'reg_lambda': 3.752560127114259}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:59:12,491] Trial 53 finished with value: 0.3487655269410598 and parameters: {'num_leaves': 112, 'min_child_samples': 88, 'learning_rate': 0.19329549141289415, 'n_estimators': 112, 'subsample': 0.9436078141989165, 'colsample_bytree': 0.9957275435103055, 'reg_alpha': 1.0412750521519287e-07, 'reg_lambda': 1.1751039758309043}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:59:19,591] Trial 54 finished with value: 0.34745429757222324 and parameters: {'num_leaves': 105, 'min_child_samples': 81, 'learning_rate': 0.2596829125171214, 'n_estimators': 63, 'subsample': 0.998608243398778, 'colsample_bytree': 0.93686655264257, 'reg_alpha': 0.8718159322561422, 'reg_lambda': 0.39822395755439083}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:59:26,770] Trial 55 finished with value: 0.35014201396109024 and parameters: {'num_leaves': 124, 'min_child_samples': 89, 'learning_rate': 0.16789063001614324, 'n_estimators': 51, 'subsample': 0.9798829228008693, 'colsample_bytree': 0.8689765974867623, 'reg_alpha': 3.714931414208539e-06, 'reg_lambda': 9.752256389942195}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:59:39,758] Trial 56 finished with value: 0.3431403941622223 and parameters: {'num_leaves': 100, 'min_child_samples': 73, 'learning_rate': 0.28212248079918206, 'n_estimators': 127, 'subsample': 0.9245922149549721, 'colsample_bytree': 0.9070937363207399, 'reg_alpha': 4.231616541825187, 'reg_lambda': 3.7089140202131605}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 06:59:52,753] Trial 57 finished with value: 0.34342102715404743 and parameters: {'num_leaves': 99, 'min_child_samples': 72, 'learning_rate': 0.28524889172416124, 'n_estimators': 135, 'subsample': 0.9240227037405594, 'colsample_bytree': 0.9017864013764886, 'reg_alpha': 2.2116650535228186, 'reg_lambda': 3.7257024248633415}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 07:00:05,340] Trial 58 finished with value: 0.3447500185474872 and parameters: {'num_leaves': 83, 'min_child_samples': 75, 'learning_rate': 0.2991256920638431, 'n_estimators': 152, 'subsample': 0.8189109538685754, 'colsample_bytree': 0.9332672124896555, 'reg_alpha': 8.002837717874962e-05, 'reg_lambda': 4.48889929658588}. Best is trial 41 with value: 0.34288142037420966.\n[I 2025-11-28 07:00:20,760] Trial 59 finished with value: 0.3468169159519183 and parameters: {'num_leaves': 98, 'min_child_samples': 69, 'learning_rate': 0.277262906592288, 'n_estimators': 181, 'subsample': 0.9268189701919458, 'colsample_bytree': 0.8327077596668533, 'reg_alpha': 0.4660459628703516, 'reg_lambda': 4.5365058722731115}. Best is trial 41 with value: 0.34288142037420966.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035743 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032553 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036876 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045137 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010987 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019454 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037334 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039786 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037189 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037117 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019668 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036323 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037271 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037839 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035692 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036104 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036818 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:00:32,179] Trial 60 finished with value: 0.3521327840587759 and parameters: {'num_leaves': 75, 'min_child_samples': 65, 'learning_rate': 0.09162480139746447, 'n_estimators': 133, 'subsample': 0.9075922566485118, 'colsample_bytree': 0.8474911418691712, 'reg_alpha': 1.9505512988804872, 'reg_lambda': 0.35322223713188117}. Best is trial 41 with value: 0.34288142037420966.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036846 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037350 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019450 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035471 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034800 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018446 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034530 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036309 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018032 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010634 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018193 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035205 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017885 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034278 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017904 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036167 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:00:43,524] Trial 61 finished with value: 0.34032837918702996 and parameters: {'num_leaves': 92, 'min_child_samples': 94, 'learning_rate': 0.25045367847909994, 'n_estimators': 115, 'subsample': 0.9529515087150552, 'colsample_bytree': 0.900959734103827, 'reg_alpha': 5.905000114813205, 'reg_lambda': 2.3034543546174433}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:00:55,216] Trial 62 finished with value: 0.34335118925142305 and parameters: {'num_leaves': 94, 'min_child_samples': 94, 'learning_rate': 0.2727614620365413, 'n_estimators': 116, 'subsample': 0.6924830003356016, 'colsample_bytree': 0.9042158563964322, 'reg_alpha': 5.102854245232571, 'reg_lambda': 4.279735949979552}. Best is trial 61 with value: 0.34032837918702996.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038017 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039514 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038939 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039619 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038361 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038138 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038135 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039072 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039273 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040336 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036751 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038623 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039893 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039121 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019567 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:01:06,396] Trial 63 finished with value: 0.34682177750738585 and parameters: {'num_leaves': 92, 'min_child_samples': 94, 'learning_rate': 0.27998647003981186, 'n_estimators': 123, 'subsample': 0.6741137688293573, 'colsample_bytree': 0.8785467436391264, 'reg_alpha': 1.1560311494236306, 'reg_lambda': 5.054740415718698}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:01:27,578] Trial 64 finished with value: 0.34218863251301823 and parameters: {'num_leaves': 87, 'min_child_samples': 83, 'learning_rate': 0.2722642147306096, 'n_estimators': 246, 'subsample': 0.725337919249504, 'colsample_bytree': 0.9039792707892551, 'reg_alpha': 5.725708998136799, 'reg_lambda': 2.3292123615290308}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:01:47,329] Trial 65 finished with value: 0.34423509171036926 and parameters: {'num_leaves': 88, 'min_child_samples': 82, 'learning_rate': 0.2010779818425732, 'n_estimators': 253, 'subsample': 0.7298952269298569, 'colsample_bytree': 0.7871445278135829, 'reg_alpha': 5.23602291187662, 'reg_lambda': 1.8943849830686392}. Best is trial 61 with value: 0.34032837918702996.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019805 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037951 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038135 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038427 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038417 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020083 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038980 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038367 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037215 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039163 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037499 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019715 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039492 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037143 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038936 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038440 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036653 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:02:18,233] Trial 66 finished with value: 0.34414325706371396 and parameters: {'num_leaves': 74, 'min_child_samples': 98, 'learning_rate': 0.24917745892200982, 'n_estimators': 440, 'subsample': 0.701116149623907, 'colsample_bytree': 0.8230915734761333, 'reg_alpha': 2.4235387627926634, 'reg_lambda': 9.522441500842294}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:02:33,390] Trial 67 finished with value: 0.3437068407908701 and parameters: {'num_leaves': 84, 'min_child_samples': 94, 'learning_rate': 0.2678740541750852, 'n_estimators': 170, 'subsample': 0.647852543393386, 'colsample_bytree': 0.9546987810336806, 'reg_alpha': 6.120974404320246, 'reg_lambda': 0.12735303806482345}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:02:42,165] Trial 68 finished with value: 0.34777497200992014 and parameters: {'num_leaves': 94, 'min_child_samples': 85, 'learning_rate': 0.22303596043487056, 'n_estimators': 88, 'subsample': 0.7561685647409486, 'colsample_bytree': 0.9787039034806396, 'reg_alpha': 0.47318879676821946, 'reg_lambda': 0.8733888314392101}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:03:05,247] Trial 69 finished with value: 0.359111885361046 and parameters: {'num_leaves': 67, 'min_child_samples': 78, 'learning_rate': 0.03853630088956892, 'n_estimators': 296, 'subsample': 0.7205738757921942, 'colsample_bytree': 0.8652218797044142, 'reg_alpha': 0.1424135090226109, 'reg_lambda': 3.2659158872525938e-06}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:03:26,490] Trial 70 finished with value: 0.3450958009671784 and parameters: {'num_leaves': 103, 'min_child_samples': 92, 'learning_rate': 0.24665047653360736, 'n_estimators': 246, 'subsample': 0.699834692375504, 'colsample_bytree': 0.9183010411565856, 'reg_alpha': 1.2414563149345892, 'reg_lambda': 0.00016899619396542503}. Best is trial 61 with value: 0.34032837918702996.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035458 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036796 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035121 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017744 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036082 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017688 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035025 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017968 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037989 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018022 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035977 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035856 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018336 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010903 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018438 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033908 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:03:40,182] Trial 71 finished with value: 0.344297662465211 and parameters: {'num_leaves': 100, 'min_child_samples': 70, 'learning_rate': 0.2856491309232422, 'n_estimators': 143, 'subsample': 0.9512095098141462, 'colsample_bytree': 0.89868114862815, 'reg_alpha': 2.6909507535621224, 'reg_lambda': 2.3280222364024676}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:03:50,533] Trial 72 finished with value: 0.34063279597347984 and parameters: {'num_leaves': 78, 'min_child_samples': 78, 'learning_rate': 0.29731545737885695, 'n_estimators': 114, 'subsample': 0.9884525554187811, 'colsample_bytree': 0.909116555751429, 'reg_alpha': 4.73516338442365, 'reg_lambda': 5.5025938130188425}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:04:00,980] Trial 73 finished with value: 0.34200665908888916 and parameters: {'num_leaves': 79, 'min_child_samples': 78, 'learning_rate': 0.26495126361171933, 'n_estimators': 116, 'subsample': 0.9846221000515608, 'colsample_bytree': 0.879779227471567, 'reg_alpha': 5.3576138927936, 'reg_lambda': 5.920943714787057}. Best is trial 61 with value: 0.34032837918702996.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035419 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037055 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036391 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019526 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036524 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037182 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011345 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036878 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037004 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005641 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035557 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036622 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039016 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038585 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037962 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037378 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:04:09,390] Trial 74 finished with value: 0.3434778911853128 and parameters: {'num_leaves': 79, 'min_child_samples': 79, 'learning_rate': 0.2999353730601661, 'n_estimators': 86, 'subsample': 0.9865563474118653, 'colsample_bytree': 0.8569632360783349, 'reg_alpha': 9.897176840060155, 'reg_lambda': 5.981681124711304}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:04:21,847] Trial 75 finished with value: 0.3441737012152658 and parameters: {'num_leaves': 72, 'min_child_samples': 77, 'learning_rate': 0.2566272789466457, 'n_estimators': 158, 'subsample': 0.9991610897741712, 'colsample_bytree': 0.8803121616606102, 'reg_alpha': 5.494722737795176, 'reg_lambda': 1.5632611191263566}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:04:34,957] Trial 76 finished with value: 0.3488989472906159 and parameters: {'num_leaves': 61, 'min_child_samples': 59, 'learning_rate': 0.22425107947236886, 'n_estimators': 196, 'subsample': 0.9422914521642409, 'colsample_bytree': 0.9275121402823457, 'reg_alpha': 1.099900544386882, 'reg_lambda': 0.6297930436223222}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:04:50,785] Trial 77 finished with value: 0.346212645167014 and parameters: {'num_leaves': 82, 'min_child_samples': 83, 'learning_rate': 0.24359667653908795, 'n_estimators': 226, 'subsample': 0.9829072821650984, 'colsample_bytree': 0.8064334839565918, 'reg_alpha': 0.05567186077090713, 'reg_lambda': 2.4740799687168376}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:05:00,715] Trial 78 finished with value: 0.3471643005897187 and parameters: {'num_leaves': 88, 'min_child_samples': 66, 'learning_rate': 0.18761245325578846, 'n_estimators': 97, 'subsample': 0.9573819551255582, 'colsample_bytree': 0.9532485687182709, 'reg_alpha': 0.4878072942504673, 'reg_lambda': 6.583058820615138}. Best is trial 61 with value: 0.34032837918702996.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037730 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040234 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035796 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037969 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037509 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036837 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036292 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021742 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039358 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037163 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019731 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036089 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036927 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018649 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036359 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010436 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036488 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039475 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038746 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035878 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037544 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041640 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037226 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038015 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037289 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037478 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036338 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035478 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036743 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037581 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037907 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038368 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:05:10,127] Trial 79 finished with value: 0.345958936051857 and parameters: {'num_leaves': 71, 'min_child_samples': 85, 'learning_rate': 0.2630012373003222, 'n_estimators': 111, 'subsample': 0.9711518134987379, 'colsample_bytree': 0.9792479048511002, 'reg_alpha': 2.4781709043347733, 'reg_lambda': 1.3135453865505402}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:05:18,023] Trial 80 finished with value: 0.3467682109184166 and parameters: {'num_leaves': 78, 'min_child_samples': 22, 'learning_rate': 0.20783332192049847, 'n_estimators': 80, 'subsample': 0.9359915337280518, 'colsample_bytree': 0.8844353994779606, 'reg_alpha': 5.576036618864216, 'reg_lambda': 0.20571175068384046}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:05:29,304] Trial 81 finished with value: 0.3424674081170712 and parameters: {'num_leaves': 86, 'min_child_samples': 96, 'learning_rate': 0.2821617918658902, 'n_estimators': 121, 'subsample': 0.7788506827133017, 'colsample_bytree': 0.909023540284219, 'reg_alpha': 5.211425262385414, 'reg_lambda': 3.040566223424309}. Best is trial 61 with value: 0.34032837918702996.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035896 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018378 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034902 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.040538 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018169 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034590 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037791 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018310 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033970 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035623 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018741 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037949 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035982 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018074 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036008 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018167 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:05:42,384] Trial 82 finished with value: 0.3474333609924718 and parameters: {'num_leaves': 91, 'min_child_samples': 7, 'learning_rate': 0.23643085900930813, 'n_estimators': 129, 'subsample': 0.9562188820393311, 'colsample_bytree': 0.8454625489892917, 'reg_alpha': 1.431879557810485, 'reg_lambda': 9.868274851888076}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:05:53,952] Trial 83 finished with value: 0.3433954083000449 and parameters: {'num_leaves': 63, 'min_child_samples': 76, 'learning_rate': 0.2807129787817263, 'n_estimators': 149, 'subsample': 0.9917939704526285, 'colsample_bytree': 0.9495554454494028, 'reg_alpha': 5.989729036971919, 'reg_lambda': 2.3749782251034333}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:06:03,538] Trial 84 finished with value: 0.34563528540465127 and parameters: {'num_leaves': 86, 'min_child_samples': 12, 'learning_rate': 0.25566769893054686, 'n_estimators': 96, 'subsample': 0.8965482931886516, 'colsample_bytree': 0.9057089989661228, 'reg_alpha': 2.633619346193197, 'reg_lambda': 0.4851640838742689}. Best is trial 61 with value: 0.34032837918702996.\n[I 2025-11-28 07:06:15,184] Trial 85 finished with value: 0.34029798535268463 and parameters: {'num_leaves': 84, 'min_child_samples': 71, 'learning_rate': 0.2685365891316399, 'n_estimators': 120, 'subsample': 0.7689608928109108, 'colsample_bytree': 0.9171046004255292, 'reg_alpha': 9.389300139120559, 'reg_lambda': 5.520416734345424}. Best is trial 85 with value: 0.34029798535268463.\n[I 2025-11-28 07:06:30,834] Trial 86 finished with value: 0.34075809545777425 and parameters: {'num_leaves': 85, 'min_child_samples': 73, 'learning_rate': 0.29881972007870183, 'n_estimators': 174, 'subsample': 0.7771472168903972, 'colsample_bytree': 0.9267197165567643, 'reg_alpha': 8.843395119663695, 'reg_lambda': 1.5396708927586463e-07}. Best is trial 85 with value: 0.34029798535268463.\n[I 2025-11-28 07:06:45,806] Trial 87 finished with value: 0.34058845239529056 and parameters: {'num_leaves': 80, 'min_child_samples': 73, 'learning_rate': 0.2996942115337434, 'n_estimators': 171, 'subsample': 0.7839729802622102, 'colsample_bytree': 0.9282379240023021, 'reg_alpha': 9.9934216838203, 'reg_lambda': 2.8360901159451798e-08}. Best is trial 85 with value: 0.34029798535268463.\n[I 2025-11-28 07:07:02,451] Trial 88 finished with value: 0.35115404019476415 and parameters: {'num_leaves': 77, 'min_child_samples': 69, 'learning_rate': 0.05561562837576515, 'n_estimators': 170, 'subsample': 0.7783712401698157, 'colsample_bytree': 0.9267567029224154, 'reg_alpha': 9.530020617588203, 'reg_lambda': 2.5631286338475777e-08}. Best is trial 85 with value: 0.34029798535268463.\n[I 2025-11-28 07:07:20,381] Trial 89 finished with value: 0.3462864833569157 and parameters: {'num_leaves': 85, 'min_child_samples': 80, 'learning_rate': 0.29633256260954977, 'n_estimators': 221, 'subsample': 0.8143859069836293, 'colsample_bytree': 0.9879376072933612, 'reg_alpha': 1.6676528853116388, 'reg_lambda': 1.259837683066018e-07}. Best is trial 85 with value: 0.34029798535268463.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036831 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037761 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037841 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036793 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037844 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038119 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019333 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010761 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039364 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036983 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039943 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036964 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038319 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039655 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037587 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035069 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011137 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018928 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034561 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018124 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010868 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018407 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038033 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017904 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036997 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018484 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037995 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018935 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037109 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005765 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037166 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030171 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:07:38,378] Trial 90 finished with value: 0.36341492754250837 and parameters: {'num_leaves': 69, 'min_child_samples': 100, 'learning_rate': 0.030005071596438695, 'n_estimators': 192, 'subsample': 0.7836668711591713, 'colsample_bytree': 0.8894084014436439, 'reg_alpha': 0.8370211824092175, 'reg_lambda': 2.062610822353217e-07}. Best is trial 85 with value: 0.34029798535268463.\n[I 2025-11-28 07:07:51,362] Trial 91 finished with value: 0.34329888723704016 and parameters: {'num_leaves': 80, 'min_child_samples': 75, 'learning_rate': 0.2796889388186378, 'n_estimators': 160, 'subsample': 0.7482402329464157, 'colsample_bytree': 0.9069862464454147, 'reg_alpha': 3.085407053070585, 'reg_lambda': 5.539123751048258e-08}. Best is trial 85 with value: 0.34029798535268463.\n[I 2025-11-28 07:08:03,821] Trial 92 finished with value: 0.3424829541182669 and parameters: {'num_leaves': 81, 'min_child_samples': 72, 'learning_rate': 0.2891964839949472, 'n_estimators': 141, 'subsample': 0.7607368723910366, 'colsample_bytree': 0.9241275903686764, 'reg_alpha': 6.402272442321308, 'reg_lambda': 1.4286607099659524e-08}. Best is trial 85 with value: 0.34029798535268463.\n[I 2025-11-28 07:08:13,796] Trial 93 finished with value: 0.34538817969122926 and parameters: {'num_leaves': 76, 'min_child_samples': 67, 'learning_rate': 0.23563597873468936, 'n_estimators': 110, 'subsample': 0.7994815445941228, 'colsample_bytree': 0.9245737583968998, 'reg_alpha': 7.152789922865597, 'reg_lambda': 1.5309049414183233e-08}. Best is trial 85 with value: 0.34029798535268463.\n[I 2025-11-28 07:08:30,256] Trial 94 finished with value: 0.34386068337706144 and parameters: {'num_leaves': 87, 'min_child_samples': 61, 'learning_rate': 0.2664835736919541, 'n_estimators': 178, 'subsample': 0.7614303462614103, 'colsample_bytree': 0.9579968703037608, 'reg_alpha': 9.242153252526466, 'reg_lambda': 4.0347261207784513e-07}. Best is trial 85 with value: 0.34029798535268463.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020422 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037214 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037245 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037881 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039113 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038807 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037104 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038267 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038336 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037076 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039596 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038105 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038240 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039119 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036314 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036852 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.043000 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:08:50,396] Trial 95 finished with value: 0.3429184444426887 and parameters: {'num_leaves': 82, 'min_child_samples': 72, 'learning_rate': 0.29122136263713555, 'n_estimators': 262, 'subsample': 0.7418677525885268, 'colsample_bytree': 0.8733969311150274, 'reg_alpha': 3.650919182433814, 'reg_lambda': 3.927991083557594e-08}. Best is trial 85 with value: 0.34029798535268463.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037225 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039660 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037618 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038117 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036550 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036733 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036421 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.039583 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037261 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037927 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037586 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037422 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037141 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036810 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036544 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036787 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036497 seconds.\nYou can set `force_col_wise=true` to remove the overhead.","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:08:59,025] Trial 96 finished with value: 0.34681620183346074 and parameters: {'num_leaves': 44, 'min_child_samples': 97, 'learning_rate': 0.2546602429247544, 'n_estimators': 137, 'subsample': 0.723086743435382, 'colsample_bytree': 0.9396003234076723, 'reg_alpha': 1.7990341443176419, 'reg_lambda': 2.421976757860675e-08}. Best is trial 85 with value: 0.34029798535268463.\n[I 2025-11-28 07:09:11,296] Trial 97 finished with value: 0.3381045616806586 and parameters: {'num_leaves': 70, 'min_child_samples': 50, 'learning_rate': 0.299156108531212, 'n_estimators': 151, 'subsample': 0.7838482131531566, 'colsample_bytree': 0.9174826176446363, 'reg_alpha': 9.993983178238395, 'reg_lambda': 7.530055719606666e-08}. Best is trial 97 with value: 0.3381045616806586.\n[I 2025-11-28 07:09:24,380] Trial 98 finished with value: 0.34189567476223537 and parameters: {'num_leaves': 80, 'min_child_samples': 85, 'learning_rate': 0.298958594913991, 'n_estimators': 152, 'subsample': 0.7858056508154567, 'colsample_bytree': 0.9168185979170811, 'reg_alpha': 5.9418491139137775, 'reg_lambda': 7.771603337185437e-08}. Best is trial 97 with value: 0.3381045616806586.\n[I 2025-11-28 07:09:34,746] Trial 99 finished with value: 0.34830946285768083 and parameters: {'num_leaves': 64, 'min_child_samples': 86, 'learning_rate': 0.24748420533868024, 'n_estimators': 151, 'subsample': 0.801582898355903, 'colsample_bytree': 0.8461632350767809, 'reg_alpha': 3.189246347320374, 'reg_lambda': 1.1750081649205522e-06}. Best is trial 97 with value: 0.3381045616806586.\n","output_type":"stream"},{"name":"stdout","text":"Best LGB params: {'num_leaves': 70, 'min_child_samples': 50, 'learning_rate': 0.299156108531212, 'n_estimators': 151, 'subsample': 0.7838482131531566, 'colsample_bytree': 0.9174826176446363, 'reg_alpha': 9.993983178238395, 'reg_lambda': 7.530055719606666e-08} best f2: 0.6618954383193414\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def objective_cat(trial):\n    params = {\n        \"iterations\": trial.suggest_int(\"iterations\", 200, 3000),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.03, 0.3, log = True),\n        \"depth\": trial.suggest_int(\"depth\", 3, 10),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-8, 10.0, log = True),\n        \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n        \"random_state\": 42,\n        \"verbose\": 0,\n        \"thread_count\" = -1\n    }\n\n    model = CatBoostClassifier(**params)\n    scores = cross_val_score(model, X_selected, y, cv = cv, scoring = fbeta_scorer, n_jobs = -1)\n    return 1.0 - scores.mean()\n\nstudy_cat = optuna.create_study(direction = \"minimize\", sampler = optuna.samplers.TPESampler(seed = 42))\nstudy_cat.optimize(objective_cat, n_trials = 100)\nprint(\"Best Cat params:\", study_cat.best_params, \"best f2:\", 1 - study_cat.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T07:09:34.751289Z","iopub.execute_input":"2025-11-28T07:09:34.751555Z","iopub.status.idle":"2025-11-28T12:28:57.713193Z","shell.execute_reply.started":"2025-11-28T07:09:34.751536Z","shell.execute_reply":"2025-11-28T12:28:57.712250Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"[I 2025-11-28 07:09:34,756] A new study created in memory with name: no-name-c399e419-560d-4035-806e-3cd005fd75a3\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018675 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038720 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036077 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019474 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035737 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035438 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018661 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035410 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018256 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036735 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017900 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036402 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018312 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034272 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037085 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021157 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034771 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017545 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034450 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034099 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018250 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 07:12:13,513] Trial 0 finished with value: 0.36406536275185974 and parameters: {'iterations': 1249, 'learning_rate': 0.2678154091306088, 'depth': 8, 'l2_leaf_reg': 0.0024430162614261413, 'border_count': 66}. Best is trial 0 with value: 0.36406536275185974.\n[I 2025-11-28 07:14:35,761] Trial 1 finished with value: 0.36781489391792055 and parameters: {'iterations': 636, 'learning_rate': 0.034292951628939645, 'depth': 9, 'l2_leaf_reg': 0.002570603566117598, 'border_count': 190}. Best is trial 0 with value: 0.36406536275185974.\n[I 2025-11-28 07:15:19,953] Trial 2 finished with value: 0.37689675283712165 and parameters: {'iterations': 257, 'learning_rate': 0.27991818073277003, 'depth': 9, 'l2_leaf_reg': 8.148018307012941e-07, 'border_count': 72}. Best is trial 0 with value: 0.36406536275185974.\n[I 2025-11-28 07:16:38,279] Trial 3 finished with value: 0.3655929343607658 and parameters: {'iterations': 713, 'learning_rate': 0.060445433652475976, 'depth': 7, 'l2_leaf_reg': 7.71800699380605e-05, 'border_count': 97}. Best is trial 0 with value: 0.36406536275185974.\n[I 2025-11-28 07:19:09,800] Trial 4 finished with value: 0.3615169922967061 and parameters: {'iterations': 1913, 'learning_rate': 0.0413632938580613, 'depth': 5, 'l2_leaf_reg': 1.9826980964985924e-05, 'border_count': 134}. Best is trial 4 with value: 0.3615169922967061.\n[I 2025-11-28 07:23:13,611] Trial 5 finished with value: 0.37158583102410414 and parameters: {'iterations': 2399, 'learning_rate': 0.04751109467735623, 'depth': 7, 'l2_leaf_reg': 0.0021465011216654484, 'border_count': 42}. Best is trial 4 with value: 0.3615169922967061.\n[I 2025-11-28 07:25:17,613] Trial 6 finished with value: 0.36336709005732915 and parameters: {'iterations': 1901, 'learning_rate': 0.044426835359925546, 'depth': 3, 'l2_leaf_reg': 3.4671276804481113, 'border_count': 248}. Best is trial 4 with value: 0.3615169922967061.\n[I 2025-11-28 07:27:45,170] Trial 7 finished with value: 0.3582564372265036 and parameters: {'iterations': 2464, 'learning_rate': 0.06049716507542575, 'depth': 3, 'l2_leaf_reg': 0.014391207615728067, 'border_count': 130}. Best is trial 7 with value: 0.3582564372265036.\n[I 2025-11-28 07:28:19,061] Trial 8 finished with value: 0.3746563331181988 and parameters: {'iterations': 541, 'learning_rate': 0.0938205911034111, 'depth': 3, 'l2_leaf_reg': 1.527156759251193, 'border_count': 89}. Best is trial 7 with value: 0.3582564372265036.\n[I 2025-11-28 07:31:57,387] Trial 9 finished with value: 0.36379357010400126 and parameters: {'iterations': 2055, 'learning_rate': 0.06149394156285053, 'depth': 7, 'l2_leaf_reg': 0.0008325158565947976, 'border_count': 73}. Best is trial 7 with value: 0.3582564372265036.\n[I 2025-11-28 07:35:55,482] Trial 10 finished with value: 0.3473887070590076 and parameters: {'iterations': 2934, 'learning_rate': 0.1503129037649601, 'depth': 5, 'l2_leaf_reg': 1.4509648127973841e-08, 'border_count': 175}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 07:39:41,101] Trial 11 finished with value: 0.35107529805978077 and parameters: {'iterations': 2820, 'learning_rate': 0.1485359558408426, 'depth': 5, 'l2_leaf_reg': 5.892039038704306e-08, 'border_count': 173}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 07:43:44,911] Trial 12 finished with value: 0.34810081785125235 and parameters: {'iterations': 2979, 'learning_rate': 0.1676931960647724, 'depth': 5, 'l2_leaf_reg': 2.0121970487147716e-08, 'border_count': 192}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 07:47:48,954] Trial 13 finished with value: 0.3496674500837764 and parameters: {'iterations': 2916, 'learning_rate': 0.16233284498946124, 'depth': 5, 'l2_leaf_reg': 1.0935528520312723e-08, 'border_count': 216}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 07:51:13,871] Trial 14 finished with value: 0.34839424825936105 and parameters: {'iterations': 2541, 'learning_rate': 0.1606612565029248, 'depth': 5, 'l2_leaf_reg': 6.715294874634983e-07, 'border_count': 172}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 07:52:49,697] Trial 15 finished with value: 0.35660305062877407 and parameters: {'iterations': 1356, 'learning_rate': 0.10942717215514149, 'depth': 4, 'l2_leaf_reg': 1.430533219004853e-06, 'border_count': 231}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 07:57:41,915] Trial 16 finished with value: 0.35231371552344437 and parameters: {'iterations': 2999, 'learning_rate': 0.218767150243085, 'depth': 6, 'l2_leaf_reg': 1.511602195404429e-08, 'border_count': 195}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:01:54,624] Trial 17 finished with value: 0.3514329167928717 and parameters: {'iterations': 2668, 'learning_rate': 0.10763426884511175, 'depth': 6, 'l2_leaf_reg': 1.7754789920320172e-07, 'border_count': 163}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:04:33,319] Trial 18 finished with value: 0.3494111919472702 and parameters: {'iterations': 2257, 'learning_rate': 0.21121895080972003, 'depth': 4, 'l2_leaf_reg': 8.50066195704687e-06, 'border_count': 215}. Best is trial 10 with value: 0.3473887070590076.\nTraining has stopped (degenerate solution on iteration 219, probably too small l2-regularization, try to increase it)\nTraining has stopped (degenerate solution on iteration 294, probably too small l2-regularization, try to increase it)\n[I 2025-11-28 08:09:49,010] Trial 19 finished with value: 0.3917435067944779 and parameters: {'iterations': 1608, 'learning_rate': 0.1376865345751015, 'depth': 10, 'l2_leaf_reg': 1.2768064787846076e-07, 'border_count': 145}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:12:17,673] Trial 20 finished with value: 0.35584059503343013 and parameters: {'iterations': 2199, 'learning_rate': 0.08314248374949924, 'depth': 4, 'l2_leaf_reg': 0.10070504597414726, 'border_count': 119}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:15:46,431] Trial 21 finished with value: 0.3500776145428378 and parameters: {'iterations': 2629, 'learning_rate': 0.19077216029824523, 'depth': 5, 'l2_leaf_reg': 1.047617044644066e-06, 'border_count': 161}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:20:02,304] Trial 22 finished with value: 0.35274299674708853 and parameters: {'iterations': 2634, 'learning_rate': 0.12872926372540372, 'depth': 6, 'l2_leaf_reg': 5.848308434653919e-08, 'border_count': 183}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:23:18,202] Trial 23 finished with value: 0.349033505328077 and parameters: {'iterations': 2810, 'learning_rate': 0.1755277386020466, 'depth': 4, 'l2_leaf_reg': 7.567026876394415e-06, 'border_count': 203}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:27:18,294] Trial 24 finished with value: 0.3508448460689302 and parameters: {'iterations': 3000, 'learning_rate': 0.12797340007804917, 'depth': 5, 'l2_leaf_reg': 2.9408123954350244e-07, 'border_count': 155}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:31:09,246] Trial 25 finished with value: 0.35324559189125493 and parameters: {'iterations': 2399, 'learning_rate': 0.2509644236723286, 'depth': 6, 'l2_leaf_reg': 1.0666393646258126e-08, 'border_count': 175}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:34:19,950] Trial 26 finished with value: 0.35310536556902516 and parameters: {'iterations': 2643, 'learning_rate': 0.0760664312721294, 'depth': 4, 'l2_leaf_reg': 3.468130552766302e-06, 'border_count': 254}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:38:08,032] Trial 27 finished with value: 0.34760501181396164 and parameters: {'iterations': 2761, 'learning_rate': 0.19745283183617313, 'depth': 5, 'l2_leaf_reg': 5.129192589665145e-05, 'border_count': 210}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:42:53,006] Trial 28 finished with value: 0.34887164027869244 and parameters: {'iterations': 2828, 'learning_rate': 0.23051555701989634, 'depth': 6, 'l2_leaf_reg': 0.12434946096841705, 'border_count': 232}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:45:48,221] Trial 29 finished with value: 0.36212023787871395 and parameters: {'iterations': 1092, 'learning_rate': 0.19180725115979758, 'depth': 8, 'l2_leaf_reg': 0.000104133116311454, 'border_count': 205}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:51:56,458] Trial 30 finished with value: 0.3638884864007108 and parameters: {'iterations': 2236, 'learning_rate': 0.1186221878295281, 'depth': 8, 'l2_leaf_reg': 6.944754940439474e-05, 'border_count': 230}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:55:17,280] Trial 31 finished with value: 0.3498749724511996 and parameters: {'iterations': 2514, 'learning_rate': 0.16348148415852154, 'depth': 5, 'l2_leaf_reg': 3.577329017948918e-08, 'border_count': 180}. Best is trial 10 with value: 0.3473887070590076.\n[I 2025-11-28 08:58:58,751] Trial 32 finished with value: 0.34566874328193875 and parameters: {'iterations': 2744, 'learning_rate': 0.2928077114751319, 'depth': 5, 'l2_leaf_reg': 4.91821405053113e-07, 'border_count': 192}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:02:15,299] Trial 33 finished with value: 0.34828239342800216 and parameters: {'iterations': 2761, 'learning_rate': 0.28348904214461235, 'depth': 4, 'l2_leaf_reg': 3.0136504165176916e-07, 'border_count': 196}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:07:10,401] Trial 34 finished with value: 0.3510825003080317 and parameters: {'iterations': 2999, 'learning_rate': 0.2541801230518334, 'depth': 6, 'l2_leaf_reg': 3.8006526868447103e-08, 'border_count': 214}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:10:56,609] Trial 35 finished with value: 0.35037067922497234 and parameters: {'iterations': 2778, 'learning_rate': 0.2859173868697668, 'depth': 5, 'l2_leaf_reg': 0.00025645609440117943, 'border_count': 188}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:14:15,612] Trial 36 finished with value: 0.35520597538662246 and parameters: {'iterations': 1729, 'learning_rate': 0.19868215449111318, 'depth': 7, 'l2_leaf_reg': 2.601261072497856e-05, 'border_count': 147}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:17:00,477] Trial 37 finished with value: 0.3624979263568959 and parameters: {'iterations': 2298, 'learning_rate': 0.0306436396770049, 'depth': 4, 'l2_leaf_reg': 2.709552638641247e-06, 'border_count': 239}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:19:08,994] Trial 38 finished with value: 0.3484215213170707 and parameters: {'iterations': 2084, 'learning_rate': 0.23855656316265492, 'depth': 3, 'l2_leaf_reg': 1.0834453136378241e-07, 'border_count': 220}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:19:33,926] Trial 39 finished with value: 0.35709741154573393 and parameters: {'iterations': 200, 'learning_rate': 0.17969170378587546, 'depth': 7, 'l2_leaf_reg': 5.04963403702206e-07, 'border_count': 204}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:20:54,538] Trial 40 finished with value: 0.3539025613916409 and parameters: {'iterations': 1016, 'learning_rate': 0.145297711294476, 'depth': 5, 'l2_leaf_reg': 2.9413482180353006e-08, 'border_count': 134}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:24:09,828] Trial 41 finished with value: 0.34873502565287906 and parameters: {'iterations': 2753, 'learning_rate': 0.2610003638084605, 'depth': 4, 'l2_leaf_reg': 2.905216277287416e-07, 'border_count': 193}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:27:04,368] Trial 42 finished with value: 0.3467084842392958 and parameters: {'iterations': 2873, 'learning_rate': 0.28238376746357513, 'depth': 3, 'l2_leaf_reg': 9.564193881808645e-08, 'border_count': 197}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:29:58,461] Trial 43 finished with value: 0.34716969444747936 and parameters: {'iterations': 2866, 'learning_rate': 0.28833572205819874, 'depth': 3, 'l2_leaf_reg': 0.007382409242917888, 'border_count': 162}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:32:28,195] Trial 44 finished with value: 0.3483445103666213 and parameters: {'iterations': 2510, 'learning_rate': 0.28988213392676593, 'depth': 3, 'l2_leaf_reg': 0.010826809940987264, 'border_count': 119}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:34:54,069] Trial 45 finished with value: 0.3498915592132875 and parameters: {'iterations': 2387, 'learning_rate': 0.23840249499791552, 'depth': 3, 'l2_leaf_reg': 0.0030330497205709288, 'border_count': 166}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:37:45,710] Trial 46 finished with value: 0.3474531574141094 and parameters: {'iterations': 2848, 'learning_rate': 0.21295408045467484, 'depth': 3, 'l2_leaf_reg': 0.25223213143756334, 'border_count': 153}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:40:38,764] Trial 47 finished with value: 0.3505045643284532 and parameters: {'iterations': 2891, 'learning_rate': 0.21416130577700707, 'depth': 3, 'l2_leaf_reg': 0.4047614379950385, 'border_count': 149}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:43:18,766] Trial 48 finished with value: 0.34785033093709694 and parameters: {'iterations': 2601, 'learning_rate': 0.2929574187149089, 'depth': 3, 'l2_leaf_reg': 5.127594595179675, 'border_count': 126}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:43:50,887] Trial 49 finished with value: 0.36088280375718984 and parameters: {'iterations': 522, 'learning_rate': 0.2627989838634168, 'depth': 3, 'l2_leaf_reg': 0.03743549977270677, 'border_count': 110}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:46:46,172] Trial 50 finished with value: 0.3476714777140565 and parameters: {'iterations': 2862, 'learning_rate': 0.22478462499173552, 'depth': 3, 'l2_leaf_reg': 1.0201416447762337, 'border_count': 137}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:49:53,271] Trial 51 finished with value: 0.346439291477864 and parameters: {'iterations': 2690, 'learning_rate': 0.2999361500610851, 'depth': 4, 'l2_leaf_reg': 0.009879026938597022, 'border_count': 180}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:52:47,041] Trial 52 finished with value: 0.3616201624128591 and parameters: {'iterations': 2714, 'learning_rate': 0.24914696506069517, 'depth': 4, 'l2_leaf_reg': 0.005637185423925109, 'border_count': 42}. Best is trial 32 with value: 0.34566874328193875.\n[I 2025-11-28 09:55:43,076] Trial 53 finished with value: 0.34475501761646377 and parameters: {'iterations': 2892, 'learning_rate': 0.2970875954203717, 'depth': 3, 'l2_leaf_reg': 0.0009175531725380702, 'border_count': 170}. Best is trial 53 with value: 0.34475501761646377.\n[I 2025-11-28 09:59:00,206] Trial 54 finished with value: 0.34705805787975574 and parameters: {'iterations': 2916, 'learning_rate': 0.2955249405752225, 'depth': 4, 'l2_leaf_reg': 0.0008396098545551107, 'border_count': 169}. Best is trial 53 with value: 0.34475501761646377.\n[I 2025-11-28 10:01:54,280] Trial 55 finished with value: 0.3465766453876945 and parameters: {'iterations': 2542, 'learning_rate': 0.29273498085644517, 'depth': 4, 'l2_leaf_reg': 0.0005987037790286177, 'border_count': 167}. Best is trial 53 with value: 0.34475501761646377.\n[I 2025-11-28 10:04:43,663] Trial 56 finished with value: 0.34753700952742883 and parameters: {'iterations': 2456, 'learning_rate': 0.2652364786704347, 'depth': 4, 'l2_leaf_reg': 0.0025838897384787654, 'border_count': 184}. Best is trial 53 with value: 0.34475501761646377.\n[I 2025-11-28 10:07:07,606] Trial 57 finished with value: 0.3491485668732216 and parameters: {'iterations': 2091, 'learning_rate': 0.2756913833090815, 'depth': 4, 'l2_leaf_reg': 0.0004742355223829193, 'border_count': 171}. Best is trial 53 with value: 0.34475501761646377.\n[I 2025-11-28 10:09:19,300] Trial 58 finished with value: 0.34700283766537154 and parameters: {'iterations': 1888, 'learning_rate': 0.2966152221830148, 'depth': 4, 'l2_leaf_reg': 0.0009498047087687016, 'border_count': 179}. Best is trial 53 with value: 0.34475501761646377.\n[I 2025-11-28 10:11:18,472] Trial 59 finished with value: 0.37269357416359095 and parameters: {'iterations': 1825, 'learning_rate': 0.05497754759161365, 'depth': 4, 'l2_leaf_reg': 0.026991778931915164, 'border_count': 32}. Best is trial 53 with value: 0.34475501761646377.\n[I 2025-11-28 10:13:19,037] Trial 60 finished with value: 0.35140013694470595 and parameters: {'iterations': 1957, 'learning_rate': 0.2599318582549791, 'depth': 3, 'l2_leaf_reg': 0.0011973369776307366, 'border_count': 178}. Best is trial 53 with value: 0.34475501761646377.\n[I 2025-11-28 10:16:17,376] Trial 61 finished with value: 0.3431158803158195 and parameters: {'iterations': 2577, 'learning_rate': 0.2969267956922033, 'depth': 4, 'l2_leaf_reg': 0.0007228750065433437, 'border_count': 169}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:17:50,443] Trial 62 finished with value: 0.3495713251979644 and parameters: {'iterations': 1329, 'learning_rate': 0.29662590944544054, 'depth': 4, 'l2_leaf_reg': 0.000220130976603484, 'border_count': 196}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:20:33,929] Trial 63 finished with value: 0.34577375786966735 and parameters: {'iterations': 2400, 'learning_rate': 0.2353232062528887, 'depth': 4, 'l2_leaf_reg': 0.00045042667867965097, 'border_count': 156}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:23:08,689] Trial 64 finished with value: 0.34760127750773273 and parameters: {'iterations': 2557, 'learning_rate': 0.2375479741883818, 'depth': 3, 'l2_leaf_reg': 0.0001584080663097899, 'border_count': 157}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:26:17,108] Trial 65 finished with value: 0.34541113139905233 and parameters: {'iterations': 2669, 'learning_rate': 0.2709689155344809, 'depth': 4, 'l2_leaf_reg': 0.0018886200953213644, 'border_count': 189}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:29:26,248] Trial 66 finished with value: 0.34758923913748396 and parameters: {'iterations': 2365, 'learning_rate': 0.2293097088372147, 'depth': 5, 'l2_leaf_reg': 0.0005892416936248481, 'border_count': 142}. Best is trial 61 with value: 0.3431158803158195.\n","output_type":"stream"},{"name":"stdout","text":"\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037841 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035232 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038275 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038458 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041620 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037434 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036106 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036106 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036387 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037284 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.037935 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.036823 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-28 10:42:23,738] Trial 67 finished with value: 0.37136451720154573 and parameters: {'iterations': 2653, 'learning_rate': 0.24879120796378146, 'depth': 10, 'l2_leaf_reg': 0.001986919224235607, 'border_count': 187}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:45:18,441] Trial 68 finished with value: 0.346323037303106 and parameters: {'iterations': 2161, 'learning_rate': 0.2654330922713878, 'depth': 5, 'l2_leaf_reg': 0.004032583462468385, 'border_count': 160}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:48:26,453] Trial 69 finished with value: 0.34872684486109407 and parameters: {'iterations': 2321, 'learning_rate': 0.20918887885005333, 'depth': 5, 'l2_leaf_reg': 0.02981002933668723, 'border_count': 156}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:51:42,310] Trial 70 finished with value: 0.34770122163531725 and parameters: {'iterations': 2425, 'learning_rate': 0.17733198566646918, 'depth': 5, 'l2_leaf_reg': 0.004569698946586, 'border_count': 174}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:54:48,255] Trial 71 finished with value: 0.34696033225134837 and parameters: {'iterations': 2705, 'learning_rate': 0.2707322454988478, 'depth': 4, 'l2_leaf_reg': 0.0003977653261175955, 'border_count': 160}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 10:57:45,993] Trial 72 finished with value: 0.34750776474850575 and parameters: {'iterations': 2558, 'learning_rate': 0.27165106430844693, 'depth': 4, 'l2_leaf_reg': 0.012396540902036112, 'border_count': 168}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:00:45,093] Trial 73 finished with value: 0.3473881919382148 and parameters: {'iterations': 2184, 'learning_rate': 0.2472499419433336, 'depth': 5, 'l2_leaf_reg': 0.0014736748551565238, 'border_count': 185}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:03:30,619] Trial 74 finished with value: 0.34787734720071284 and parameters: {'iterations': 2455, 'learning_rate': 0.22565456048569496, 'depth': 4, 'l2_leaf_reg': 0.00011358995388007925, 'border_count': 142}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:06:00,223] Trial 75 finished with value: 0.34758416525504543 and parameters: {'iterations': 2169, 'learning_rate': 0.2712249353446254, 'depth': 4, 'l2_leaf_reg': 3.1358874224358886e-05, 'border_count': 152}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:09:28,983] Trial 76 finished with value: 0.3589511851747267 and parameters: {'iterations': 2596, 'learning_rate': 0.038739957784459995, 'depth': 5, 'l2_leaf_reg': 0.0003796662778371144, 'border_count': 177}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:13:42,867] Trial 77 finished with value: 0.35181183472560185 and parameters: {'iterations': 2697, 'learning_rate': 0.09001322693474015, 'depth': 6, 'l2_leaf_reg': 0.004169281845327494, 'border_count': 165}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:16:36,379] Trial 78 finished with value: 0.3466388605993128 and parameters: {'iterations': 2499, 'learning_rate': 0.24387158329298933, 'depth': 4, 'l2_leaf_reg': 1.4848923781536237e-05, 'border_count': 189}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:19:32,566] Trial 79 finished with value: 0.3618568852176096 and parameters: {'iterations': 2346, 'learning_rate': 0.06932881772518987, 'depth': 5, 'l2_leaf_reg': 0.017582060621538203, 'border_count': 66}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:29:19,378] Trial 80 finished with value: 0.3563669854375453 and parameters: {'iterations': 2785, 'learning_rate': 0.2595170224085357, 'depth': 9, 'l2_leaf_reg': 0.05282609279564434, 'border_count': 221}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:32:16,396] Trial 81 finished with value: 0.34729135929515187 and parameters: {'iterations': 2529, 'learning_rate': 0.24119098920620624, 'depth': 4, 'l2_leaf_reg': 0.00066812063202689, 'border_count': 190}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:35:13,321] Trial 82 finished with value: 0.3461518430694591 and parameters: {'iterations': 2496, 'learning_rate': 0.2994171068359144, 'depth': 4, 'l2_leaf_reg': 8.395286644296125e-06, 'border_count': 200}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:37:53,117] Trial 83 finished with value: 0.34662739931565245 and parameters: {'iterations': 2248, 'learning_rate': 0.277285605231148, 'depth': 4, 'l2_leaf_reg': 0.00022907212028443013, 'border_count': 209}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:40:57,649] Trial 84 finished with value: 0.35025563855740427 and parameters: {'iterations': 2634, 'learning_rate': 0.2997292676265746, 'depth': 4, 'l2_leaf_reg': 0.0016155599658292605, 'border_count': 199}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:44:38,422] Trial 85 finished with value: 0.34956887697523387 and parameters: {'iterations': 2732, 'learning_rate': 0.20347269834984474, 'depth': 5, 'l2_leaf_reg': 0.008984760692850795, 'border_count': 181}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:48:34,752] Trial 86 finished with value: 0.351496514592039 and parameters: {'iterations': 2950, 'learning_rate': 0.2762212466606455, 'depth': 5, 'l2_leaf_reg': 4.522494584406908e-05, 'border_count': 162}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:52:39,595] Trial 87 finished with value: 0.3536552815481041 and parameters: {'iterations': 2580, 'learning_rate': 0.22061528719514592, 'depth': 6, 'l2_leaf_reg': 4.051325138695728e-06, 'border_count': 171}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:55:07,273] Trial 88 finished with value: 0.3477378606091073 and parameters: {'iterations': 2423, 'learning_rate': 0.2578802417806897, 'depth': 3, 'l2_leaf_reg': 0.003390370413125804, 'border_count': 204}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 11:57:44,628] Trial 89 finished with value: 0.34803835135950245 and parameters: {'iterations': 2300, 'learning_rate': 0.18664793745474956, 'depth': 4, 'l2_leaf_reg': 1.697081967365347e-06, 'border_count': 148}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:01:02,595] Trial 90 finished with value: 0.34483875088184 and parameters: {'iterations': 2817, 'learning_rate': 0.2995378927760542, 'depth': 4, 'l2_leaf_reg': 8.916609434349378e-05, 'border_count': 193}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:04:06,650] Trial 91 finished with value: 0.3461392887132222 and parameters: {'iterations': 2645, 'learning_rate': 0.29970573257692823, 'depth': 4, 'l2_leaf_reg': 7.918262211387035e-05, 'border_count': 201}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:07:22,216] Trial 92 finished with value: 0.34851558726098464 and parameters: {'iterations': 2798, 'learning_rate': 0.28300415823492175, 'depth': 4, 'l2_leaf_reg': 1.0695811679611934e-05, 'border_count': 201}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:10:31,642] Trial 93 finished with value: 0.345581499164198 and parameters: {'iterations': 2677, 'learning_rate': 0.2733973636627453, 'depth': 4, 'l2_leaf_reg': 5.5408180775675064e-05, 'border_count': 192}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:14:23,265] Trial 94 finished with value: 0.35028982378746976 and parameters: {'iterations': 2816, 'learning_rate': 0.23169855947810805, 'depth': 5, 'l2_leaf_reg': 0.00011189578079440457, 'border_count': 210}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:17:21,748] Trial 95 finished with value: 0.34629045183861373 and parameters: {'iterations': 2905, 'learning_rate': 0.27009440204213764, 'depth': 3, 'l2_leaf_reg': 4.5561269717320235e-05, 'border_count': 223}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:20:22,381] Trial 96 finished with value: 0.34539087604600405 and parameters: {'iterations': 2934, 'learning_rate': 0.25225307589667845, 'depth': 3, 'l2_leaf_reg': 6.88795062378623e-05, 'border_count': 225}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:23:24,645] Trial 97 finished with value: 0.3465940627896902 and parameters: {'iterations': 2953, 'learning_rate': 0.2480652726074074, 'depth': 3, 'l2_leaf_reg': 6.846631051129016e-05, 'border_count': 235}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:26:09,367] Trial 98 finished with value: 0.34670746094872074 and parameters: {'iterations': 2675, 'learning_rate': 0.2821657466584944, 'depth': 3, 'l2_leaf_reg': 2.565225510770461e-05, 'border_count': 227}. Best is trial 61 with value: 0.3431158803158195.\n[I 2025-11-28 12:28:57,707] Trial 99 finished with value: 0.35118527192910987 and parameters: {'iterations': 2752, 'learning_rate': 0.254583602529048, 'depth': 3, 'l2_leaf_reg': 1.588078834806901e-05, 'border_count': 215}. Best is trial 61 with value: 0.3431158803158195.\n","output_type":"stream"},{"name":"stdout","text":"Best Cat params: {'iterations': 2577, 'learning_rate': 0.2969267956922033, 'depth': 4, 'l2_leaf_reg': 0.0007228750065433437, 'border_count': 169} best f2: 0.6568841196841805\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nimport copy\n\ndef get_oof_preds(model, X, y, cv):\n    oof_preds = np.zeros((X.shape[0], len(np.unique(y))))  # For predict_proba\n    models = []\n    \n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        \n        clf = copy.deepcopy(model)\n        clf.fit(X_train, y_train)\n        oof_preds[val_idx] = clf.predict_proba(X_val)\n        models.append(clf)\n        \n    return oof_preds, models\n\n# Prepare CV\ncv = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\n# Prepare sanitized base models\nxgb_model = XGBClassifier(**sanitize_xgb_params(study_xgb.best_params))\nlgb_model = LGBMClassifier(**sanitize_lgb_params(study_lgb.best_params))\ncat_model = CatBoostClassifier(**sanitize_cat_params(study_cat.best_params))\n\n# Generate OOF predictions\nxgb_oof, xgb_models = get_oof_preds(xgb_model, X_selected, y, cv)\nlgb_oof, lgb_models = get_oof_preds(lgb_model, X_selected, y, cv)\ncat_oof, cat_models = get_oof_preds(cat_model, X_selected, y, cv)\n\n# Stack them as features for the final model\nX_blender = np.hstack([xgb_oof, lgb_oof, cat_oof])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T21:08:48.304802Z","iopub.execute_input":"2025-11-28T21:08:48.305270Z","iopub.status.idle":"2025-11-28T21:12:04.503383Z","shell.execute_reply.started":"2025-11-28T21:08:48.305243Z","shell.execute_reply":"2025-11-28T21:12:04.502252Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 27405, number of negative: 50793\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014308 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350457 -> initscore=-0.617033\n[LightGBM] [Info] Start training from score -0.617033\n[LightGBM] [Info] Number of positive: 27302, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012492 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349139 -> initscore=-0.622824\n[LightGBM] [Info] Start training from score -0.622824\n[LightGBM] [Info] Number of positive: 27335, number of negative: 50863\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012718 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349561 -> initscore=-0.620968\n[LightGBM] [Info] Start training from score -0.620968\n[LightGBM] [Info] Number of positive: 27135, number of negative: 51064\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012699 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.346999 -> initscore=-0.632255\n[LightGBM] [Info] Start training from score -0.632255\n[LightGBM] [Info] Number of positive: 27339, number of negative: 50860\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019711 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349608 -> initscore=-0.620763\n[LightGBM] [Info] Start training from score -0.620763\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"def objective_final(trial):\n    C = trial.suggest_float(\"C\", 1e-4, 1e2, log=True)\n    penalty = trial.suggest_categorical(\"penalty\", [\"l2\"])\n    \n    final_clf = LogisticRegression(\n        C=C,\n        penalty=penalty,\n        solver=\"saga\",\n        max_iter=2000,\n        random_state=42\n    )\n    \n    scores = cross_val_score(final_clf, X_blender, y, cv=cv, scoring=fbeta_scorer, n_jobs=-1)\n    return 1.0 - scores.mean()\n\nstudy_final = optuna.create_study(direction = \"minimize\", sampler=optuna.samplers.TPESampler(seed = 42))\nstudy_final.optimize(objective_final, n_trials = 50)\n\nprint(\"Best final model params:\", study_final.best_params)\nprint(\"Best F2 score:\", 1 - study_final.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T21:12:04.505015Z","iopub.execute_input":"2025-11-28T21:12:04.505540Z","iopub.status.idle":"2025-11-28T21:13:18.606668Z","shell.execute_reply.started":"2025-11-28T21:12:04.505515Z","shell.execute_reply":"2025-11-28T21:13:18.605007Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"[I 2025-11-28 21:12:04,510] A new study created in memory with name: no-name-7cecceb0-8dad-497e-b126-f7bedeafc8d8\n[I 2025-11-28 21:12:07,861] Trial 0 finished with value: 0.3430137414057878 and parameters: {'C': 0.017670169402947963, 'penalty': 'l2'}. Best is trial 0 with value: 0.3430137414057878.\n[I 2025-11-28 21:12:09,670] Trial 1 finished with value: 0.3416462166265658 and parameters: {'C': 50.61576888752309, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:11,225] Trial 2 finished with value: 0.34166390261157376 and parameters: {'C': 2.465832945854912, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:12,615] Trial 3 finished with value: 0.3416532588780783 and parameters: {'C': 0.39079671568228835, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:14,045] Trial 4 finished with value: 0.3640599369402593 and parameters: {'C': 0.0008632008168602544, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:15,446] Trial 5 finished with value: 0.36410966220805796 and parameters: {'C': 0.0008629132190071859, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:16,805] Trial 6 finished with value: 0.40151681915636284 and parameters: {'C': 0.00022310108018679258, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:18,411] Trial 7 finished with value: 0.3416462166265658 and parameters: {'C': 15.741890047456648, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:19,878] Trial 8 finished with value: 0.3416571074931144 and parameters: {'C': 0.4042872735027334, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:21,442] Trial 9 finished with value: 0.3416600552935549 and parameters: {'C': 1.7718847354806828, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:23,107] Trial 10 finished with value: 0.3416462166265658 and parameters: {'C': 75.33019084841487, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:24,740] Trial 11 finished with value: 0.3416462166265658 and parameters: {'C': 56.61867321769047, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:26,329] Trial 12 finished with value: 0.3416462166265658 and parameters: {'C': 7.896242244767498, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:27,902] Trial 13 finished with value: 0.3416462166265658 and parameters: {'C': 16.70943782430828, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:29,406] Trial 14 finished with value: 0.34229992308590784 and parameters: {'C': 0.025924453821513007, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:31,009] Trial 15 finished with value: 0.3416462166265658 and parameters: {'C': 14.446231960135087, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:32,608] Trial 16 finished with value: 0.3416462166265658 and parameters: {'C': 79.53409633698588, 'penalty': 'l2'}. Best is trial 1 with value: 0.3416462166265658.\n[I 2025-11-28 21:12:34,056] Trial 17 finished with value: 0.34163843151521545 and parameters: {'C': 1.0986672238048356, 'penalty': 'l2'}. Best is trial 17 with value: 0.34163843151521545.\n[I 2025-11-28 21:12:35,447] Trial 18 finished with value: 0.3416042588726729 and parameters: {'C': 0.09744986017189518, 'penalty': 'l2'}. Best is trial 18 with value: 0.3416042588726729.\n[I 2025-11-28 21:12:36,813] Trial 19 finished with value: 0.34298903211555654 and parameters: {'C': 0.018950708900499894, 'penalty': 'l2'}. Best is trial 18 with value: 0.3416042588726729.\n[I 2025-11-28 21:12:38,163] Trial 20 finished with value: 0.34161700567488607 and parameters: {'C': 0.07548196962164401, 'penalty': 'l2'}. Best is trial 18 with value: 0.3416042588726729.\n[I 2025-11-28 21:12:39,636] Trial 21 finished with value: 0.34155933387424753 and parameters: {'C': 0.10988242550104883, 'penalty': 'l2'}. Best is trial 21 with value: 0.34155933387424753.\n[I 2025-11-28 21:12:40,999] Trial 22 finished with value: 0.34162116752320526 and parameters: {'C': 0.08775815484322212, 'penalty': 'l2'}. Best is trial 21 with value: 0.34155933387424753.\n[I 2025-11-28 21:12:42,374] Trial 23 finished with value: 0.34161700567488607 and parameters: {'C': 0.07561947201762492, 'penalty': 'l2'}. Best is trial 21 with value: 0.34155933387424753.\n[I 2025-11-28 21:12:43,754] Trial 24 finished with value: 0.3467824147113532 and parameters: {'C': 0.004869835818011959, 'penalty': 'l2'}. Best is trial 21 with value: 0.34155933387424753.\n[I 2025-11-28 21:12:45,106] Trial 25 finished with value: 0.34159782588633203 and parameters: {'C': 0.25184565932401715, 'penalty': 'l2'}. Best is trial 21 with value: 0.34155933387424753.\n[I 2025-11-28 21:12:46,455] Trial 26 finished with value: 0.34159782588633203 and parameters: {'C': 0.27381589759663366, 'penalty': 'l2'}. Best is trial 21 with value: 0.34155933387424753.\n[I 2025-11-28 21:12:47,854] Trial 27 finished with value: 0.3417061232355042 and parameters: {'C': 0.3548965810943915, 'penalty': 'l2'}. Best is trial 21 with value: 0.34155933387424753.\n[I 2025-11-28 21:12:49,291] Trial 28 finished with value: 0.3449514967569426 and parameters: {'C': 0.0072993090051721, 'penalty': 'l2'}. Best is trial 21 with value: 0.34155933387424753.\n[I 2025-11-28 21:12:50,633] Trial 29 finished with value: 0.3415498726795754 and parameters: {'C': 0.21443573563495275, 'penalty': 'l2'}. Best is trial 29 with value: 0.3415498726795754.\n[I 2025-11-28 21:12:51,978] Trial 30 finished with value: 0.34187544402870695 and parameters: {'C': 0.037192502846458815, 'penalty': 'l2'}. Best is trial 29 with value: 0.3415498726795754.\n[I 2025-11-28 21:12:53,325] Trial 31 finished with value: 0.34160189402236296 and parameters: {'C': 0.22936931422020673, 'penalty': 'l2'}. Best is trial 29 with value: 0.3415498726795754.\n[I 2025-11-28 21:12:54,706] Trial 32 finished with value: 0.34163843151521545 and parameters: {'C': 1.3036863959504075, 'penalty': 'l2'}. Best is trial 29 with value: 0.3415498726795754.\n[I 2025-11-28 21:12:56,051] Trial 33 finished with value: 0.34152360282856264 and parameters: {'C': 0.207382692884503, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:12:57,384] Trial 34 finished with value: 0.34164245434702933 and parameters: {'C': 0.762471689059296, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:12:58,818] Trial 35 finished with value: 0.3416462166265658 and parameters: {'C': 3.5321205381376086, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:00,148] Trial 36 finished with value: 0.34159859808867454 and parameters: {'C': 0.18369107716135855, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:01,519] Trial 37 finished with value: 0.34495601611039395 and parameters: {'C': 0.007457653115511255, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:02,868] Trial 38 finished with value: 0.34173255929652924 and parameters: {'C': 0.04122959666287313, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:04,216] Trial 39 finished with value: 0.35301754087606463 and parameters: {'C': 0.001972630923027012, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:05,640] Trial 40 finished with value: 0.3416308530288965 and parameters: {'C': 0.6576230817526733, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:07,049] Trial 41 finished with value: 0.3415498726795754 and parameters: {'C': 0.1995383023306761, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:08,453] Trial 42 finished with value: 0.3415861085773866 and parameters: {'C': 0.16848869345572073, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:09,905] Trial 43 finished with value: 0.3416123231031404 and parameters: {'C': 0.14968179712010898, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:11,406] Trial 44 finished with value: 0.3416462166265658 and parameters: {'C': 3.6640936381287, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:12,900] Trial 45 finished with value: 0.34164245434702933 and parameters: {'C': 0.7379757848965997, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:14,323] Trial 46 finished with value: 0.34168660307225895 and parameters: {'C': 0.04983263924948642, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:15,720] Trial 47 finished with value: 0.34165280733217407 and parameters: {'C': 0.48628510434873995, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:17,082] Trial 48 finished with value: 0.3416388014435734 and parameters: {'C': 0.1384023612494678, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n[I 2025-11-28 21:13:18,601] Trial 49 finished with value: 0.3416600552935549 and parameters: {'C': 2.0933625384325216, 'penalty': 'l2'}. Best is trial 33 with value: 0.34152360282856264.\n","output_type":"stream"},{"name":"stdout","text":"Best final model params: {'C': 0.207382692884503, 'penalty': 'l2'}\nBest F2 score: 0.6584763971714374\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"print(study_xgb.best_params)\nprint(study_lgb.best_params)\nprint(study_cat.best_params)\n\nprint(study_blender.best_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T21:13:18.607694Z","iopub.execute_input":"2025-11-28T21:13:18.608042Z","iopub.status.idle":"2025-11-28T21:13:18.619048Z","shell.execute_reply.started":"2025-11-28T21:13:18.608012Z","shell.execute_reply":"2025-11-28T21:13:18.617879Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"{'n_estimators': 491, 'max_depth': 5, 'learning_rate': 0.21686268622292826, 'subsample': 0.5525800241980569, 'colsample_bytree': 0.5864315116436489, 'gamma': 1.459068523957759, 'reg_alpha': 0.0005270933696306975, 'reg_lambda': 0.07852312052461549}\n{'num_leaves': 70, 'min_child_samples': 50, 'learning_rate': 0.299156108531212, 'n_estimators': 151, 'subsample': 0.7838482131531566, 'colsample_bytree': 0.9174826176446363, 'reg_alpha': 9.993983178238395, 'reg_lambda': 7.530055719606666e-08}\n{'iterations': 2577, 'learning_rate': 0.2969267956922033, 'depth': 4, 'l2_leaf_reg': 0.0007228750065433437, 'border_count': 169}\n{'C': 0.017670169402947963, 'penalty': 'l2'}\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"## Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\n\nxgb_best = study_xgb.best_params\nlgb_best = study_lgb.best_params\ncat_best = study_cat.best_params\nblender_best = study_blender.best_params\n\n# --- Base models ---\nxgb_final = XGBClassifier(\n    objective = \"binary:logistic\",\n    eval_metric = \"logloss\",\n    use_label_encoder = False,\n    random_state = 42,\n    n_jobs = 1,\n    **xgb_best\n)\n\nlgb_final = LGBMClassifier(\n    objective = \"binary\",\n    random_state = 42,\n    n_jobs = 1,\n    **lgb_best\n)\n\ncat_final = CatBoostClassifier(\n    loss_function = \"Logloss\",\n    eval_metric = \"Logloss\",\n    verbose = 0,\n    random_state = 42,\n    thread_count = 1,\n    **cat_best\n)\n\nblender = LogisticRegression(\n    solver = \"saga\",\n    max_iter = 5000,\n    random_state = 42,\n    **blender_best\n)\n\n# --- Stacking classifier ---\nestimators = [\n    (\"XGBoost\", xgb_final),\n    (\"LightGBM\", lgb_final),\n    (\"CatBoost\", cat_final)\n]\n\nstack = StackingClassifier(\n    estimators = estimators,\n    final_estimator = blender,\n    stack_method = \"predict_proba\",\n    cv = 5,\n    n_jobs = -1,\n    passthrough = False\n)\n\n# --- Cross-validated predictions ---\ncv = KFold(n_splits = 5, shuffle = True, random_state = 42)\ny_proba_oof = cross_val_predict(stack, X_selected, y, cv = cv, \n                                method = \"predict_proba\", n_jobs = -1)[:, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T21:36:13.191929Z","iopub.execute_input":"2025-11-28T21:36:13.192548Z","iopub.status.idle":"2025-11-28T21:52:19.631336Z","shell.execute_reply.started":"2025-11-28T21:36:13.192515Z","shell.execute_reply":"2025-11-28T21:52:19.630285Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 27135, number of negative: 51064\n[LightGBM] [Info] Number of positive: 27335, number of negative: 50863\n[LightGBM] [Info] Number of positive: 27302, number of negative: 50896\n[LightGBM] [Info] Number of positive: 27405, number of negative: 50793\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114457 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.346999 -> initscore=-0.632255\n[LightGBM] [Info] Start training from score -0.632255\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031565 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349561 -> initscore=-0.620968\n[LightGBM] [Info] Start training from score -0.620968\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041621 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349139 -> initscore=-0.622824\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.122933 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Start training from score -0.622824\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350457 -> initscore=-0.617033\n[LightGBM] [Info] Start training from score -0.617033\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40716\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092701 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622786\n[LightGBM] [Info] Start training from score -0.622786\n[LightGBM] [Info] Number of positive: 21868, number of negative: 40690\n[LightGBM] [Info] Number of positive: 21924, number of negative: 40634\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.095844 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.089646 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349564 -> initscore=-0.620958\n[LightGBM] [Info] Start training from score -0.620958\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350459 -> initscore=-0.617023\n[LightGBM] [Info] Start training from score -0.617023\n[LightGBM] [Info] Number of positive: 21708, number of negative: 40851\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.097269 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.347000 -> initscore=-0.632250\n[LightGBM] [Info] Start training from score -0.632250\n[LightGBM] [Info] Number of positive: 21868, number of negative: 40690\n[LightGBM] [Info] Number of positive: 21924, number of negative: 40634\n[LightGBM] [Info] Number of positive: 21841, number of negative: 40717\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027402 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.096755 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350459 -> initscore=-0.617023\n[LightGBM] [Info] Start training from score -0.617023\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349564 -> initscore=-0.620958\n[LightGBM] [Info] Start training from score -0.620958\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.106796 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349132 -> initscore=-0.622857\n[LightGBM] [Info] Start training from score -0.622857\n[LightGBM] [Info] Number of positive: 21708, number of negative: 40851\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.044940 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.347000 -> initscore=-0.632250\n[LightGBM] [Info] Start training from score -0.632250\n[LightGBM] [Info] Number of positive: 21868, number of negative: 40690\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.099030 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349564 -> initscore=-0.620958\n[LightGBM] [Info] Start training from score -0.620958\n[LightGBM] [Info] Number of positive: 21924, number of negative: 40634\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027040 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350459 -> initscore=-0.617023\n[LightGBM] [Info] Start training from score -0.617023\n[LightGBM] [Info] Number of positive: 21841, number of negative: 40717\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094317 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62558, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349132 -> initscore=-0.622857\n[LightGBM] [Info] Start training from score -0.622857\n[LightGBM] [Info] Number of positive: 21708, number of negative: 40851\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032781 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.347000 -> initscore=-0.632250\n[LightGBM] [Info] Start training from score -0.632250\n[LightGBM] [Info] Number of positive: 21868, number of negative: 40691\n[LightGBM] [Info] Number of positive: 21924, number of negative: 40635\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.094423 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349558 -> initscore=-0.620983\n[LightGBM] [Info] Start training from score -0.620983\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030081 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350453 -> initscore=-0.617048\n[LightGBM] [Info] Start training from score -0.617048\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40717\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032098 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349142 -> initscore=-0.622811\n[LightGBM] [Info] Start training from score -0.622811\n[LightGBM] [Info] Number of positive: 21708, number of negative: 40851\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.092905 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.347000 -> initscore=-0.632250\n[LightGBM] [Info] Start training from score -0.632250\n[LightGBM] [Info] Number of positive: 21868, number of negative: 40691\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031377 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349558 -> initscore=-0.620983\n[LightGBM] [Info] Start training from score -0.620983\n[LightGBM] [Info] Number of positive: 21924, number of negative: 40635\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.101230 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.350453 -> initscore=-0.617048\n[LightGBM] [Info] Start training from score -0.617048\n[LightGBM] [Info] Number of positive: 21842, number of negative: 40717\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.091187 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349142 -> initscore=-0.622811\n[LightGBM] [Info] Start training from score -0.622811\n[LightGBM] [Info] Number of positive: 21708, number of negative: 40852\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026743 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62560, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.346995 -> initscore=-0.632275\n[LightGBM] [Info] Start training from score -0.632275\n[LightGBM] [Info] Number of positive: 27339, number of negative: 50860\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.038573 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349608 -> initscore=-0.620763\n[LightGBM] [Info] Start training from score -0.620763\n[LightGBM] [Info] Number of positive: 21871, number of negative: 40688\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031329 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349606 -> initscore=-0.620772\n[LightGBM] [Info] Start training from score -0.620772\n[LightGBM] [Info] Number of positive: 21871, number of negative: 40688\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018036 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349606 -> initscore=-0.620772\n[LightGBM] [Info] Start training from score -0.620772\n[LightGBM] [Info] Number of positive: 21871, number of negative: 40688\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005879 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349606 -> initscore=-0.620772\n[LightGBM] [Info] Start training from score -0.620772\n[LightGBM] [Info] Number of positive: 21871, number of negative: 40688\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031167 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62559, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349606 -> initscore=-0.620772\n[LightGBM] [Info] Start training from score -0.620772\n[LightGBM] [Info] Number of positive: 21872, number of negative: 40688\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017645 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 62560, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349616 -> initscore=-0.620726\n[LightGBM] [Info] Start training from score -0.620726\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"threshold = 0.2\ny_pred_oof = (y_proba_oof >= threshold).astype(int)\n\nbeta = 2\nf2 = fbeta_score(y, y_pred_oof, beta = beta)\nprint(f\"OOF F{beta} score: {f2:.4f}\")\n\n# Confusion matrix\ncm = confusion_matrix(y, y_pred_oof)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))\ndisp.plot(cmap=\"Blues\")\nplt.title(\"OOF Confusion Matrix\")\nplt.show()\n\n# Classification report\nprint(\"OOF Classification Report:\")\nprint(classification_report(y, y_pred_oof, digits=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T21:52:19.635730Z","iopub.execute_input":"2025-11-28T21:52:19.636080Z","iopub.status.idle":"2025-11-28T21:52:20.156156Z","shell.execute_reply.started":"2025-11-28T21:52:19.636056Z","shell.execute_reply":"2025-11-28T21:52:20.155087Z"}},"outputs":[{"name":"stdout","text":"OOF F2 score: 0.8148\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"OOF Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9250    0.6695    0.7768     63619\n           1     0.5933    0.8987    0.7148     34129\n\n    accuracy                         0.7495     97748\n   macro avg     0.7591    0.7841    0.7458     97748\nweighted avg     0.8092    0.7495    0.7551     97748\n\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ninference = pd.read_csv(\"/kaggle/input/cpe342-karena/public_dataset/task1/test.csv\")\ninference.drop(columns = [\"id\", \"player_id\"], inplace = True)\n\nX_test = inference[selected_features].copy()\n\n# X_selected = X.copy()\n# X_test = inference.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:50:03.396889Z","iopub.execute_input":"2025-11-28T14:50:03.397279Z","iopub.status.idle":"2025-11-28T14:50:03.755893Z","shell.execute_reply.started":"2025-11-28T14:50:03.397255Z","shell.execute_reply":"2025-11-28T14:50:03.754990Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"xgb_best = study_xgb.best_params\nlgb_best = study_lgb.best_params\ncat_best = study_cat.best_params\nblender_best = study_blender.best_params\n\nxgb_final = XGBClassifier(\n    objective = \"binary:logistic\",\n    eval_metric = \"logloss\",\n    use_label_encoder = False,\n    random_state = 42,\n    n_jobs = 1,\n    **xgb_best\n)\n\nlgb_final = LGBMClassifier(\n    objective = \"binary\",\n    random_state = 42,\n    n_jobs = 1,\n    **lgb_best\n)\n\ncat_final = CatBoostClassifier(\n    loss_function = \"Logloss\",\n    eval_metric = \"Logloss\",\n    verbose = 0,\n    random_state = 42,\n    thread_count = 1,\n    **cat_best\n)\n\nblender = LogisticRegression(\n    solver = \"saga\",\n    max_iter = 5000,\n    random_state = 42,\n    **blender_best\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:50:07.390092Z","iopub.execute_input":"2025-11-28T14:50:07.390511Z","iopub.status.idle":"2025-11-28T14:50:07.396218Z","shell.execute_reply.started":"2025-11-28T14:50:07.390488Z","shell.execute_reply":"2025-11-28T14:50:07.395300Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"estimators = [\n    (\"XGBoost\", xgb_final),\n    (\"LightGBM\", lgb_final),\n    (\"CatBoost\", cat_final)\n]\n\nstack = StackingClassifier(\n    estimators = estimators,\n    final_estimator = blender,\n    stack_method = \"predict_proba\",\n    cv = 5,\n    n_jobs = -1,\n    passthrough = False\n)\n\nprint(\"Training STACK with best params...\")\nstack.fit(X_selected, y)\nprint(\"DONE retraining with best parameters!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:50:08.635475Z","iopub.execute_input":"2025-11-28T14:50:08.635765Z","iopub.status.idle":"2025-11-28T14:52:34.533131Z","shell.execute_reply.started":"2025-11-28T14:50:08.635745Z","shell.execute_reply":"2025-11-28T14:52:34.532303Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Training STACK with best params...\n[LightGBM] [Info] Number of positive: 27304, number of negative: 50895\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50895\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.116253 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075295 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349160 -> initscore=-0.622731\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622731\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.102914 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.132910 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78198, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349152 -> initscore=-0.622768\n[LightGBM] [Info] Start training from score -0.622768\n[LightGBM] [Info] Number of positive: 27303, number of negative: 50896\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.034897 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 78199, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349148 -> initscore=-0.622788\n[LightGBM] [Info] Start training from score -0.622788\nDONE retraining with best parameters!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"probs = stack.predict_proba(X_test)[:, 1]\npreds = (probs >= 0.2).astype(int)\nprint(\"Predictions ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:52:34.534452Z","iopub.execute_input":"2025-11-28T14:52:34.535078Z","iopub.status.idle":"2025-11-28T14:52:35.495233Z","shell.execute_reply.started":"2025-11-28T14:52:34.535055Z","shell.execute_reply":"2025-11-28T14:52:35.494244Z"}},"outputs":[{"name":"stdout","text":"Predictions ready!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"submit = pd.read_csv(\"/kaggle/input/cpe342-karena/public_dataset/sample_submission.csv\")\n\nsubmit[\"task1\"] = preds\nsubmit.to_csv(\"ThreeTreeStackEnsembleHyperparameter0.2.csv\", index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T14:52:35.496183Z","iopub.execute_input":"2025-11-28T14:52:35.496482Z","iopub.status.idle":"2025-11-28T14:52:35.593766Z","shell.execute_reply.started":"2025-11-28T14:52:35.496456Z","shell.execute_reply":"2025-11-28T14:52:35.592837Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 34129, number of negative: 63619\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046188 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 5655\n[LightGBM] [Info] Number of data points in the train set: 97748, number of used features: 24\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.349153 -> initscore=-0.622765\n[LightGBM] [Info] Start training from score -0.622765\n","output_type":"stream"}],"execution_count":22}]}