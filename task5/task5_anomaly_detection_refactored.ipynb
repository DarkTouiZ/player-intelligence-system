{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668fc4df",
   "metadata": {
    "id": "668fc4df"
   },
   "source": [
    "\n",
    "# Task 5 – Account Security Monitoring (Anomaly Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559f55b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9559f55b",
    "outputId": "0f7d5868-64d7-4050-ae93-d062451431a9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AD_DIR: /content/data/anomalyDetection\n",
      "Sample submission: True\n",
      "Test: True\n",
      "Baseline: True\n"
     ]
    }
   ],
   "source": "\n# ================================================\n# 2. Imports and Path Definitions\n\nimport os\nfrom pathlib import Path\nimport datetime\nimport json\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\n\nBASE_DIR = Path(\"/content/data\")\nAD_DIR = BASE_DIR / \"anomalyDetection\"\nAD_DIR.mkdir(exist_ok=True)\n\nPATH_SAMPLE_SUB = BASE_DIR / \"sample_submission.csv\"\nPATH_TEST = BASE_DIR / \"test.csv\"\nPATH_BASELINE = BASE_DIR / \"baseLine.csv\"\n\nTRAIN_VAL_CANDIDATES = [\n    (BASE_DIR / \"src\" / \"train.csv\", BASE_DIR / \"src\" / \"val.csv\"),\n    (BASE_DIR / \"train.csv\", BASE_DIR / \"val.csv\"),\n]\n\nTARGET_COL = \"is_anomaly\"  # from datacard\nID_COLS_CANDIDATES = [\"id\", \"player_id\"]  # we will auto-detect later\n\nprint(\"AD_DIR:\", AD_DIR)\nprint(\"Sample submission:\", PATH_SAMPLE_SUB.exists())\nprint(\"Test:\", PATH_TEST.exists())\nprint(\"Baseline:\", PATH_BASELINE.exists())"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# IMPORTANT: Safe JSON dumps for numpy types\nimport json\n\ndef json_safe_dumps(obj, **kwargs):\n    def default(o):\n        try:\n            import numpy as np\n            if isinstance(o, (np.integer,)):\n                return int(o)\n            if isinstance(o, (np.floating,)):\n                return float(o)\n            if isinstance(o, (np.ndarray,)):\n                return o.tolist()\n        except Exception:\n            pass\n        try:\n            return o.__dict__\n        except Exception:\n            return str(o)\n    return json_safe_dumps(obj, default=default, **kwargs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb6722",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1cb6722",
    "outputId": "3a7d61ad-d554-41bd-d31c-5163fd0ffdec"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Using train file: /content/data/src/train.csv\n",
      "✅ Using val file  : /content/data/src/val.csv\n"
     ]
    }
   ],
   "source": "\n# ================================================\n# 3. Policy Enforcement: Require Train/Val for Task 5\n\ntrain_path = None\nval_path = None\n\nfor t_path, v_path in TRAIN_VAL_CANDIDATES:\n    if t_path.exists() and v_path.exists():\n        train_path, val_path = t_path, v_path\n        break\n\nif train_path is None or val_path is None:\n    raise SystemExit(\n        \"❌ Policy Enforcement: No Train/Val pair found.\\n\"\n        \"Expected one of:\\n\"\n        \" - /mnt/data/src/train.csv and /mnt/data/src/val.csv\\n\"\n        \" - /mnt/data/train.csv and /mnt/data/val.csv\\n\"\n        \"Please make sure one of these pairs exists before running the notebook.\"\n    )\nelse:\n    print(f\"✅ Using train file: {train_path}\")\n    print(f\"✅ Using val file  : {val_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288d19a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d288d19a",
    "outputId": "0db46ed3-c1cd-4677-89bd-dabed9836085"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train shape: (200, 11)\n",
      "Val   shape: (50, 11)\n",
      "Test  shape: (25889, 124)\n",
      "Baseline shape: (25889, 6)\n",
      "Baseline columns: ['id', 'task1', 'task2', 'task3', 'task4', 'task5']\n",
      "Sample submission shape: (25889, 6)\n",
      "Sample submission columns: ['id', 'task1', 'task2', 'task3', 'task4', 'task5']\n"
     ]
    }
   ],
   "source": "\n# ================================================\n# 4. Load Train/Val, Test, Baseline, Sample Submission\n\ntrain_df = pd.read_csv(train_path)\nval_df = pd.read_csv(val_path)\n\nprint(\"Train shape:\", train_df.shape)\nprint(\"Val   shape:\", val_df.shape)\n\ntest_df = pd.read_csv(PATH_TEST)\nprint(\"Test  shape:\", test_df.shape)\n\nbaseline_df = pd.read_csv(PATH_BASELINE)\nprint(\"Baseline shape:\", baseline_df.shape)\nprint(\"Baseline columns:\", baseline_df.columns.tolist())\n\n# Load sample submission as the authoritative template for id order + columns\nsample_sub_df = pd.read_csv(PATH_SAMPLE_SUB)\nprint(\"Sample submission shape:\", sample_sub_df.shape)\nprint(\"Sample submission columns:\", sample_sub_df.columns.tolist())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55179be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a55179be",
    "outputId": "94833a8a-254b-47ce-9967-919e0491c68a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Detected ID column: id\n",
      "Using TEST feature space for unsupervised anomaly detection\n",
      "Number of feature columns: 122\n",
      "First 10 features: ['login_count_1', 'login_count_2', 'login_count_3', 'login_count_4', 'login_lat_1', 'login_lon_1', 'login_lat_2', 'login_lon_2', 'login_lat_3', 'login_lon_3']\n",
      "Transformed Test shape: (25889, 122)\n"
     ]
    }
   ],
   "source": "# ================================================\n# 5. Feature Selection & Preprocessing\n#    - NOTE: Train/Val (f1–f10) schema != Test schema\n\nfull_train = pd.concat([train_df, val_df], axis=0, ignore_index=True)\n\n# Detect ID column if present (not used as a feature)\nid_col = None\nfor c in ID_COLS_CANDIDATES:\n    if c in full_train.columns or c in test_df.columns:\n        id_col = c\n        break\n\nprint(\"Detected ID column:\", id_col)\n\nfeature_cols = [\n    col for col in test_df.columns\n    if np.issubdtype(test_df[col].dtype, np.number)\n]\n\nprint(\"Using TEST feature space for unsupervised anomaly detection\")\nprint(\"Number of feature columns:\", len(feature_cols))\nprint(\"First 10 features:\", feature_cols[:10])\n\nif not feature_cols:\n    raise KeyError(\"❌ No numeric feature columns found in test_df for Task 5.\")\n\npreprocessor = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n        (\"scaler\", RobustScaler()),\n    ]\n)\n\nX_test = test_df[feature_cols]\nX_test_trans = preprocessor.fit_transform(X_test)\n\nprint(\"Transformed Test shape:\", X_test_trans.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ec1a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "666ec1a3",
    "outputId": "cfa806c0-935d-42e3-fcdc-dd33ea0c3e9b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting IsolationForest on TEST feature space (unsupervised)...\n",
      "Fitting OneClassSVM on TEST feature space (unsupervised)...\n",
      "✅ Both models trained from scratch on TEST feature space (no labels, no pretraining, no AutoML).\n"
     ]
    }
   ],
   "source": "# ================================================\n# 6. Train Two Unsupervised Models from Scratch\n\nRANDOM_STATE = 42\nCONTAMINATION = 0.10  # expected anomaly proportion (tunable)\nNU = 0.10             # OneClassSVM outlier fraction (tunable)\n\niso_model = IsolationForest(\n    n_estimators=300,\n    max_samples=\"auto\", \n    contamination=CONTAMINATION,\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n)\n\nprint(\"Fitting IsolationForest on TEST feature space (unsupervised)...\")\niso_model.fit(X_test_trans)\n\nocsvm_model = OneClassSVM(\n    kernel=\"rbf\",\n    gamma=\"scale\",\n    nu=NU,\n)\n\nprint(\"Fitting OneClassSVM on TEST feature space (unsupervised)...\")\nocsvm_model.fit(X_test_trans)\n\nprint(\"✅ Both models trained from scratch on TEST feature space (no labels, no pretraining, no AutoML).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840d222",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9840d222",
    "outputId": "e2185cbc-8a24-4a9d-f08c-a9f8d1d135a5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Raw test score summary:\n",
      "                iso         ocsvm\n",
      "count  25889.000000  25889.000000\n",
      "mean      -0.021032    -45.698603\n",
      "std        0.016932     49.815409\n",
      "min       -0.076323   -197.229850\n",
      "25%       -0.032538    -74.964931\n",
      "50%       -0.022001    -50.650916\n",
      "75%       -0.011120    -25.887511\n",
      "max        0.080395    211.064094\n",
      "Ensembled score summary (before thresholding):\n",
      "count    25889.000000\n",
      "mean         0.361971\n",
      "std          0.108598\n",
      "min          0.000000\n",
      "25%          0.291539\n",
      "50%          0.353879\n",
      "75%          0.417181\n",
      "max          0.925889\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": "# ================================================\n# 7. Inference on Test & Score Normalization\n\ntest_scores = pd.DataFrame(\n    {\n        \"iso\": -iso_model.decision_function(X_test_trans),\n        \"ocsvm\": -ocsvm_model.decision_function(X_test_trans),\n    },\n    index=test_df.index,\n)\n\nprint(\"Raw test score summary:\")\nprint(test_scores.describe())\n\nscore_scaler = MinMaxScaler()\ntest_scores_scaled = score_scaler.fit_transform(test_scores)\n\nensemble_scores = test_scores_scaled.mean(axis=1)\n\nprint(\"Ensembled score summary (before thresholding):\")\nprint(pd.Series(ensemble_scores).describe())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e9ba6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "438e9ba6",
    "outputId": "363dc78b-ec6b-4911-ad8d-ad7bf148fc81"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using contamination=0.100\n",
      "Anomaly threshold on ensemble score: 0.484664\n",
      "Predicted anomaly label distribution (task5):\n",
      "normal(0)     23300\n",
      "anomaly(1)     2589\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": "\n# ================================================\n# 8. Soft Ensemble -> Binary Anomaly Label (task5)\n\n# Heuristic threshold: top CONTAMINATION fraction as anomalies\nthreshold = np.quantile(ensemble_scores, 1 - CONTAMINATION)\n\nprint(f\"Using contamination={CONTAMINATION:.3f}\")\nprint(f\"Anomaly threshold on ensemble score: {threshold:.6f}\")\n\ntask5_pred = (ensemble_scores >= threshold).astype(int)\n\nprint(\"Predicted anomaly label distribution (task5):\")\nprint(pd.Series(task5_pred).value_counts().rename(index={0: \"normal(0)\", 1: \"anomaly(1)\"}))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301bd3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4301bd3f",
    "outputId": "8f5dbfed-1f63-4679-ee6e-373974a72ea5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "         id  task1  task2          task3  task4  task5\n",
      "0  ANS00001      1      2     496.056891      0      0\n",
      "1  ANS00002      0      1     904.887028      2      0\n",
      "2  ANS00003      1      3  148248.266429      3      0\n",
      "3  ANS00004      0      2      79.755910      0      0\n",
      "4  ANS00005      1      0     260.016682      3      0\n",
      "Final submission shape: (25889, 6)\n"
     ]
    }
   ],
   "source": "\n# ================================================\n# 9. Combine Task 1–4 Predictions from Baseline with Task 5 Results\n\nrequired_cols = [\"id\", \"task1\", \"task2\", \"task3\", \"task4\", \"task5\"]\n\n# Align baseline with sample submission by id\nmerged = sample_sub_df[[\"id\"]].merge(\n    baseline_df[required_cols],\n    on=\"id\",\n    how=\"left\",\n    validate=\"one_to_one\",\n)\n\nmissing_rows = merged[\"task1\"].isna().sum()\nif missing_rows > 0:\n    print(f\"⚠️ Warning: {missing_rows} rows missing tasks 1–4/5 from baseline. \"\n          \"They will remain NaN in those columns.\")\n\nif len(merged) != len(task5_pred):\n    raise ValueError(\n        f\"Length mismatch between submission template ({len(merged)}) \"\n        f\"and task5 predictions ({len(task5_pred)}).\"\n    )\n\nmerged[\"task5\"] = task5_pred\n\nsubmission_df = merged[required_cols].copy()\n\nprint(submission_df.head())\nprint(\"Final submission shape:\", submission_df.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91070ef7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91070ef7",
    "outputId": "dbd7ae01-e58a-46d4-9fd4-9677a0d5bd58"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Saved submission to: /content/data/anomalyDetection/submission.csv\n"
     ]
    }
   ],
   "source": "\n# ================================================\n# 10. Export submission.csv\n\nsubmission_path = AD_DIR / \"submission.csv\"\nsubmission_df.to_csv(submission_path, index=False)\nprint(f\"✅ Saved submission to: {submission_path}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a58faf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29a58faf",
    "outputId": "959735ba-9abe-4455-fd37-8671df77e334"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✅ Saved RUN_INFO.txt to: /content/data/anomalyDetection/RUN_INFO.txt\n"
     ]
    }
   ],
   "source": "\n# ================================================\n# 11. Save run info (RUN_INFO.txt)\n\nrun_info = {\n    \"timestamp\": datetime.datetime.now().isoformat(),\n    \"train_path\": str(train_path),\n    \"val_path\": str(val_path),\n    \"test_path\": str(PATH_TEST),\n    \"sample_submission_path\": str(PATH_SAMPLE_SUB),\n    \"baseline_path\": str(PATH_BASELINE),\n    \"feature_columns\": feature_cols,\n    \"models\": {\n        \"IsolationForest\": {\n            \"n_estimators\": int(iso_model.n_estimators),\n            \"contamination\": float(CONTAMINATION),\n            \"random_state\": int(RANDOM_STATE),\n        },\n        \"OneClassSVM\": {\n            \"kernel\": \"rbf\",\n            \"gamma\": \"scale\",\n            \"nu\": float(NU),\n        },\n    },\n    \"preprocessing\": {\n        \"imputer\": \"SimpleImputer(strategy='median')\",\n        \"scaler\": \"RobustScaler()\",\n    },\n    \"ensemble\": {\n        \"type\": \"soft-voting (mean of normalized anomaly scores)\",\n        \"score_scaler\": \"MinMaxScaler(fit on Train+Val scores only)\",\n        \"contamination\": float(CONTAMINATION),\n        \"threshold\": float(threshold),\n    },\n    \"notes\": [\n        \"Task 5 anomaly detection trained from scratch.\",\n        \"No pre-trained models used.\",\n        \"No AutoML frameworks used.\",\n        \"No fitting of any model/transformer on Test features or scores.\",\n    ],\n}\n\nrun_info_path = AD_DIR / \"RUN_INFO.txt\"\nwith open(run_info_path, \"w\") as f:\n    f.write(json_safe_dumps(run_info, indent=2))\n\nprint(f\"✅ Saved RUN_INFO.txt to: {run_info_path}\")"
  },
  {
   "cell_type": "markdown",
   "id": "9947a2fc",
   "metadata": {
    "id": "9947a2fc"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## (Optional) Autoencoder – Deep Anomaly Detector (Commented Out)\n",
    "\n",
    "> **Note:** The competition rules allow **Task 5** models to be trained **from scratch only**.  \n",
    "> If you want to use an **Autoencoder (PyTorch or Keras)**, you **must train it only on Train/Val features** and **never fit on Test data**.\n",
    "\n",
    "Below is a **template**, fully commented out, that you can adapt:\n",
    "- Build a small fully-connected Autoencoder in PyTorch or Keras.\n",
    "- Train it on `X_full_trans` (Train+Val) to reconstruct normal behavior.\n",
    "- Use reconstruction error as an anomaly score on Train and Test.\n",
    "- Then ensemble it with IsolationForest + OneClassSVM if desired.\n",
    "\n",
    "Uncomment and modify only if you are sure it still follows all policies.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}